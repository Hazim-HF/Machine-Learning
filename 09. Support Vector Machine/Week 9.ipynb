{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Support Vector Machines\n",
    "+ \"New\" learning method invented by Vladamir Vapnik in the 1990's. \n",
    "+ Likely best classifier at its time, now surpassed by gradient boosted trees and neural networks.\n",
    "\n",
    "Three different but very closely related classifiers:\n",
    "* Maximal margin classifier\n",
    "* Support Vector classifier\n",
    "* Support vector machine\n",
    "\n",
    "## Maximal Margin Classifier\n",
    "### Hyperplane\n",
    "+ An optimal hyperplane that separates classes.  \n",
    "+ **Hyperplane** - For any p dimensional space, it is a p-1 dimensional flat surface. \n",
    "+ A line in 2 dimensions, a plane in three dimensions. \n",
    "+ Mathematical definition in p dimensions: $\\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p = 0$. \n",
    "+ It divides whatever your dimension is into two pieces.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('images/pw91.png', width =700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw92.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Using a Separating Hyperplane\n",
    "+ Suppose we have  a $n \\times p$ data matrix $\\textbf{X}$\n",
    "$$x_1 = \\left[\\begin{array}{c} x_{11} \\\\ \\vdots \\\\ x_{1p} \\end{array} \\right], \\; \\ldots, \\; x_n = \\left[\\begin{array}{c} x_{n1} \\\\ \\vdots \\\\ x_{np} \\end{array} \\right]$$\n",
    "and these observations fall into two classes, $y_1, \\ldots, y_n \\in \\left\\{-1,1\\right\\}$.\n",
    "+ This approach is based upon the concept of **separating hyperplane**.\n",
    "+ Consider the left hand panel in Fig 9.2, where the separating hyperplane has the property\n",
    "$$\\begin{eqnarray} \\beta_0 + \\beta_1x_{i1}+ \\ldots + \\beta_px_{ip} > 0 & \\textrm{ if } & y_i=1 \\\\\n",
    "  \\beta_0 + \\beta_1x_{i1}+ \\ldots + \\beta_px_{ip} < 0 & \\textrm{ if } & y_i=-1 \\end{eqnarray}$$\n",
    "+ Multiplying both equations by $y_i$ yields $y_i(\\beta_0 + \\beta_1x_{i1}+ \\ldots + \\beta_px_{ip}) > 0$ for any correctly classified observation.\n",
    "+ A test observation, $x^*$, can be classified based on the sign of $f(x^*)$ (positive (class 1) or negative (class -1)).\n",
    "+ The magnitude of $f(x^*)$ can be used to measure how confident we are about the class assignment for $x^*$.\n",
    "\n",
    "### The maximal margin classifier\n",
    "+ If the data is perfectly separable then an infinite number of hyperplanes will exist that can perfectly separate the data. \n",
    "+ A natural choice is to choose a hyperplane that maximizes the distance from each observation (training) to the hyperplane - one that has a large margin - the maximum margin.\n",
    "\n",
    "### What defines maximum margin?\n",
    "+ In the linearly separable case we find the line that has the maximum margin between the two classes. \n",
    "+ The maximum margin is defined as the distance of the closet point to the separating hyperplane. \n",
    "+ So, we are maximizing the minimum distance from the hyperplane. \n",
    "+ All other points are of no consequence which is a bit scary but it happens to work well. \n",
    "+ These minimum distance points are called the **support vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw93.png', width =600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contruction of the Maximal Margin Classifier\n",
    "+ Set of training observations, $x_1, \\ldots, x_n \\in \\mathbb{R}^p$\n",
    "+ Associated class labels, $y_1, \\ldots, y_n \\in \\left\\{ -1, 1 \\right\\}$\n",
    "+ The maximal margin hyperplane is the solution to:\n",
    "$$    \\max_{\\beta_0, \\ldots, \\beta_p, M}{M} $$\n",
    "$$   \\textrm{subject to } \\sum_{j=1}^p{\\beta_j^2} = 1 $$\n",
    "$$   y_i(\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}) \\geq M \\quad \\forall i=1, \\ldots, n $$\n",
    "   where $M$ is the width of the margin.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw94.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifiers\n",
    "+ If the data is not linearly separable then no hyperplane can separate the data and thus no margin can exist. \n",
    "+ This case is most common with real data. \n",
    "+ The maximum margin classifier is very sensitive to single data points. \n",
    "+ The hyperplane can change drastically with the addition of one new data point. \n",
    "+ To help combat this type of overfitting and to allow for non-separable classification we can use a soft margin. \n",
    "+ We allow some observation to be on the wrong side of the hyperplane or within the margin. \n",
    "+ This margin violation makes the margin 'soft'.\n",
    "+ This will provide:\n",
    "    + Greater robustness to individual observations\n",
    "    + Better classification of most of the training observations\n",
    "+ Also known as **soft margin classifier**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw95.png', width =700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw96.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The hyperplane is chosen to correctly separate most of the training observations into the two classes, but may misclassify a few observations.\n",
    "+ The problem formulation is tweaked such that we allow for some total amount of error, C. \n",
    "$$    \\max_{\\beta_0, \\ldots, \\beta_p, \\epsilon_1, \\ldots , \\epsilon_n M}{M} $$\n",
    "$$   \\textrm{subject to } \\sum_{j=1}^p{\\beta_j^2} = 1 $$\n",
    "$$   y_i(\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_p x_{ip}) \\geq M(1-\\epsilon_i) $$\n",
    "$$   \\epsilon_i \\geq 0, \\quad \\sum_{i=1}^n{\\epsilon_i} \\leq C$$\n",
    "+ We seek to make $M$ as large as possible.\n",
    "+ The errors, $\\epsilon_i$,  are called slack variables, which allow observations to be on the wrong side of the margin or hyperplane. \n",
    "$$ \\begin{eqnarray} \\epsilon_i = 0 & i^\\textrm{th} \\textrm{observation on the correct side of the margin} \\\\\n",
    "\\epsilon_i >0 & i^\\textrm{th} \\textrm{observation on the wrong side of the margin} \\\\\n",
    "\\epsilon_i >1 & \\; i^\\textrm{th} \\textrm{observation on the wrong side of the hyperplane} \\end{eqnarray}$$\n",
    "+ C bounds the sum of the $\\epsilon_i$'s\n",
    "+ This total error acts as an allowance like a balance in the bank that you can spend on the amount of error you can make. \n",
    "$$ \\begin{eqnarray} C = 0 & \\textrm{no budget for violations to the margin} \\\\\n",
    "C >0 & \\textrm{  no more than C observations can be  on the wrong side of the hyperplane}  \\end{eqnarray}$$\n",
    "+ C is chosen through cross-validation.\n",
    "+ Observation that lies strictly on the correct side of the margin does not affect the support vector classifier.\n",
    "+ Observations that lie directly on the margin, or on the wrong side of the margin, are known as **support vectors**, which do affect the support vector classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw97.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# We'll define a function to draw a nice plot of an SVM\n",
    "def plot_svc(svc, X, y, h=0.02, pad=0.25):\n",
    "    x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad\n",
    "    y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
    "\n",
    "    plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=mpl.cm.Paired)\n",
    "    # Support vectors indicated in plot by vertical lines\n",
    "    sv = svc.support_vectors_\n",
    "    plt.scatter(sv[:,0], sv[:,1], c='k', marker='x', s=100, linewidths=1)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel('X1')\n",
    "    plt.ylabel('X2')\n",
    "    plt.show()\n",
    "    print('Number of support vectors: ', svc.support_.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The ${\\tt SVC()}$ function can be used to fit a support vector classifier when the argument ${\\tt kernel=\"linear\"}$ is used. \n",
    "+ This function uses a slightly different formulation of the equations we saw in lecture to build the support vector classifier. \n",
    "+ The ${\\tt c}$ argument allows us to specify the cost of a violation to the margin. When the ${\\tt c}$ argument is **small**, then the margins will be wide and many support vectors will be on the margin or will violate the margin. \n",
    "+ When the ${\\tt c}$ argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.\n",
    "\n",
    "+ We can use the ${\\tt SVC()}$ function to fit the support vector classifier for a given value of the ${\\tt cost}$ parameter. \n",
    "+ Here we demonstrate the use of this function on a two-dimensional example so that we can plot the resulting decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating random data: 20 observations of 2 features and divide into two classes.\n",
    "np.random.seed(5)\n",
    "X = np.random.randn(20,2)\n",
    "y = np.repeat([1,-1], 10)\n",
    "\n",
    "X[y == -1] = X[y == -1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data to see whether the classes are linearly separable:\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=mpl.cm.Paired)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope; not linear. Next, we fit the support vector classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1, kernel='linear')\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(svc, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The region of feature space that will be assigned to the −1 class is shown in light blue, and the region that will be assigned to the +1 class is shown in brown. \n",
    "+ The decision boundary between the two classes is linear (because we used the argument ${\\tt kernel=\"linear\"}$).\n",
    "\n",
    "+ The support vectors are plotted with crosses and the remaining observations are plotted as circles; \n",
    "+ We see here that there are 13 support vectors. \n",
    "+ We can determine their identities as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller value of the C parameter\n",
    "svc2 = SVC(C=0.1, kernel='linear')\n",
    "svc2.fit(X, y)\n",
    "plot_svc(svc2, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Now that a smaller value of the ${\\tt c}$ parameter is being used, we obtain a larger number of support vectors, because the margin is now **wider**.\n",
    "+ The ${\\tt sklearn.grid\\_search}$ module includes a a function ${\\tt GridSearchCV()}$ to perform cross-validation. \n",
    "+ In order to use this function, we pass in relevant information about the set of models that are under consideration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 10-fold cross-validation to compare SVCs with a linear kernel, using a range of values of the cost parameter:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Select the optimal C parameter by cross-validation\n",
    "tuned_parameters = [{'C': [0.001, 0.01, 0.1, 1, 5, 10, 100]}]\n",
    "clf = GridSearchCV(SVC(kernel='linear'), tuned_parameters, cv=10, scoring='accuracy')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cross-validation errors for each of these models:\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best parameters obtained\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a test data set:\n",
    "np.random.seed(1)\n",
    "X_test = np.random.randn(20,2)\n",
    "y_test = np.random.choice([-1,1], 20)\n",
    "X_test[y_test == 1] = X_test[y_test == 1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the class labels of these test observations. \n",
    "svc2 = SVC(C=0.001, kernel='linear')\n",
    "svc2.fit(X, y)\n",
    "y_pred = svc2.predict(X_test)\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred), index=svc2.classes_, columns=svc2.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 14 of the test observations are correctly classified.\n",
    "+ Consider a situation in which the two classes are linearly separable. \n",
    "+ Then we can find a separating hyperplane using the  ${\\tt svm()}$  function. \n",
    "+ First we'll give our simulated data a little nudge so that they are linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[y_test == 1] = X_test[y_test == 1] -1\n",
    "plt.scatter(X_test[:,0], X_test[:,1], s=70, c=y_test, cmap=mpl.cm.Paired)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ the observations are **just barely linearly** separable.\n",
    "+ Fit the support vector classifier and plot the resulting hyperplane, using a very large value of ${\\tt cost}$ so that no observations are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc3 = SVC(C=1e5, kernel='linear')\n",
    "svc3.fit(X_test, y_test)\n",
    "plot_svc(svc3, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ No training errors were made and only three support vectors were used.\n",
    "+ However, we can see from the figure that the margin is very narrow (because the observations that are **not** support vectors, indicated as circles, are very close to the decision boundary). \n",
    "+ It seems likely that this model will perform poorly on test data. \n",
    "+ Let's try a smaller value of ${\\tt cost}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc4 = SVC(C=1, kernel='linear')\n",
    "svc4.fit(X_test, y_test)\n",
    "plot_svc(svc4, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Using ${\\tt cost=1}$, we misclassify a training observation, but we also obtain a much wider margin and make use of five support vectors. \n",
    "+ It seems likely that this model will perform better on test data than the model with ${\\tt cost=1e5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw98.png', width =600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Classification with Non-linear Decision Boundaries\n",
    "+ For data that has a non-linear separating hyperplane, something different must be done.\n",
    "+ We can transform the variables as in previous chapters - squaring them, creating interaction terms, etc... \n",
    "+ For example, instead of fitting a support vector classifier using $p$ features,\n",
    "$$X_1,X_2, \\ldots, X_p$$\n",
    "we could instead fit using $2p$ features\n",
    "$$X_1,X_1,^2, X_2, X_2^2, \\ldots, X_p, X_p^2$$\n",
    "+ Then the opitimization problem will be\n",
    "$$    \\max_{\\beta_0, \\beta_{11}, \\beta_{12},\\ldots, \\beta_{p1}, \\beta_{p2}, \\epsilon_1, \\ldots , \\epsilon_n, M}{M} $$\n",
    "$$   \\textrm{subject to } \\sum_{j=1}^p\\sum_{k=1}^2{\\beta_{jk}^2} = 1 $$\n",
    "$$   y_i\\left(\\beta_0 + \\sum_{j=1}^p{\\beta_{j1} x_{ij}} + \\sum_{j=1}^p{\\beta_{j2} x_{ij}^2}\\right) \\geq M(1-\\epsilon_i) $$\n",
    "$$  \\epsilon_i \\geq 0, \\quad \\sum_{i=1}^n{\\epsilon_i} \\leq C $$\n",
    "  which will lead to a non-linear decision boundary.\n",
    "  \n",
    "### The SVM\n",
    "+ Extension of SVC that results from enlarging feature space by using kernels in an efficient manner without doing those transformations. \n",
    "+ The solution to SVM's involves only inner products of the observations. \n",
    "+ The **inner products** of two $r$-vectors $a$ and $b$ is defined by \n",
    "$$\\langle a, b \\rangle = \\sum_{i=1}^r{a_i b_i}$$\n",
    "Thus the innerproduct of two observations $x_i, x_{i^\\prime}$ is\n",
    "$$\\langle x_i, x_{i^\\prime} \\rangle = \\sum_{j=1}^p{x_{ij}, x_{{i^\\prime}j}}$$\n",
    "+ The linear SVC can the be represented as\n",
    "$$f(x) = \\beta_0 + \\sum_{i=1}^n{\\alpha_i \\langle x, x_i \\rangle}$$\n",
    "where there are $n$ parameters $\\alpha_i, \\; i=1, \\ldots, n$, one per training observation.\n",
    "+ To estimate these parameters, all we need are the ${n \\choose 2}$ inner products $x_i, x_{i^\\prime}$ between all pairs of training observations.\n",
    "+ $\\alpha_i$ is nonzero only for the support vectors in the solution. Therefore, we can write \n",
    "$$f(x) = \\beta_0 + \\sum_{i \\in S}{\\alpha_i \\langle x, x_i \\rangle}$$\n",
    "where $S$ is the collection of indices of those support points.\n",
    "+ The decision boundary is just a weighted sum of the inner product between observations that are the support vectors. \n",
    "+ The inner product can be replaced with a kernel function, $K(x_i, x_{i^\\prime})$, which is a function that quantifies the similarity of two observations (measure a degree of closeness). \n",
    "+ So the further the two points in the kernel function are, the smaller the result of the kernel calculation.\n",
    "+ Linear kernel is just the standard inner product. \n",
    "+ Polynomial kernel is linear kernel taken to the power of a chosen polynomial. \n",
    "$$K(x_i, x_{i^\\prime}) = (1+\\sum_{j=1}^p{x_{ij} x_{{i^\\prime}j}})^d$$\n",
    "+ The radial basis function is proportional to the squared distance between points. \n",
    "$$K(x_i, x_{i^\\prime}) = \\exp(-\\gamma\\sum_{j=1}^p{(x_{ij} -  x_{{i^\\prime}j})^2})$$\n",
    "\n",
    "+ Kernels allow for very high dimensional (infinite with radial basis function) feature space enlargement without actually going into that space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw99.png', width =600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab\n",
    "+ Fit an SVM using a **non-linear kernel** using the ${\\tt SVC()}$ function. However, now we use a different value of the parameter kernel.\n",
    "+ To fit an SVM with a polynomial kernel we use ${\\tt kernel=\"poly\"}$, and to fit an SVM with a radial kernel we use ${\\tt kernel=\"rbf\"}$. \n",
    "+ In the former case we also use the ${\\tt degree}$ argument to specify a degree for the polynomial kernel, and in the latter case we use ${\\tt gamma}$ to specify a value of $\\gamma$ for the radial basis kernel.\n",
    "\n",
    "+ Let's generate some data with a non-linear class boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(8)\n",
    "X = np.random.randn(200,2)\n",
    "X[:100] = X[:100] +2\n",
    "X[101:150] = X[101:150] -2\n",
    "y = np.concatenate([np.repeat(-1, 150), np.repeat(1,50)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=2)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=mpl.cm.Paired)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ One class is kind of stuck in the middle of another class. This suggests that we might want to use a **radial kernel** in our SVM. \n",
    "+ Fit the training data using the ${\\tt SVC()}$ function with a radial kernel and $\\gamma = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(C=1.0, kernel='rbf', gamma=1)\n",
    "svm.fit(X_train, y_train)\n",
    "plot_svc(svm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The plot shows that the resulting SVM has a decidedly non-linear boundary. \n",
    "+ There are a fair number of training errors in this SVM fit. \n",
    "+ Increase the value of cost to reduce the number of training errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increasing C parameter, allowing more flexibility\n",
    "svm2 = SVC(C=100, kernel='rbf', gamma=1.0)\n",
    "svm2.fit(X_train, y_train)\n",
    "plot_svc(svm2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ However, this comes at the price of a more irregular decision boundary that seems to be at risk of overfitting the data. \n",
    "+ We can perform cross-validation using ${\\tt GridSearchCV()}$ to select the best choice of $\\gamma$ and cost for an SVM with a radial kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'C': [0.01, 0.1, 1, 10, 100],\n",
    "                     'gamma': [0.5, 1,2,3,4]}]\n",
    "clf = GridSearchCV(SVC(kernel='rbf'), tuned_parameters, cv=10, scoring='accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The best choice of parameters involves ${\\tt cost=1}$ and ${\\tt gamma=0.5}$. \n",
    "+ Plot the resulting fit using the ${\\tt plot\\_svc()}$ function, and view the test set predictions for this model by applying the ${\\tt predict()}$ function to the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svc(clf.best_estimator_, X_test, y_test)\n",
    "print(confusion_matrix(y_test, clf.best_estimator_.predict(X_test)))\n",
    "print(clf.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85% of test observations are correctly classified by this SVM.\n",
    "\n",
    "### ROC Curves\n",
    "\n",
    "The ${\\tt auc()}$ function from the ${\\tt sklearn.metrics}$ package can be used to produce ROC curves such as those we saw in lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by fitting two models, one more flexible than the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More constrained model\n",
    "svm3 = SVC(C=1, kernel='rbf', gamma=1)\n",
    "svm3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More flexible model\n",
    "svm4 = SVC(C=1, kernel='rbf', gamma=50)\n",
    "svm4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ SVMs and SVCs output class labels for each observation.\n",
    "+ However, it is also possible to obtain fitted values for each observation, which are the numerical scores used to obtain the class labels. \n",
    "+ For instance, in the case of a SVC, the fitted value for an observation $X = (X_1,X_2, . . .,X_p)^T$ takes the form $\\hat\\beta_0 + \\hat\\beta_1X_1 + \\hat\\beta_2X_2 + . . . + \\hat\\beta_pX_p$.\n",
    "\n",
    "+ In essence, the sign of the fitted value determines on which side of the decision boundary the observation lies. \n",
    "+ Therefore, the relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero then the observation is assigned to one class, and if it is less than zero than it is assigned to the other.\n",
    "\n",
    "+ In order to obtain the fitted values for a given SVM model fit, we use the ${\\tt .decision\\_function()}$ method of the SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_score3 = svm3.decision_function(X_train)\n",
    "y_train_score4 = svm4.decision_function(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can produce the ROC plot to see how the models perform on both the training and the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_score3 = svm3.decision_function(X_train)\n",
    "y_train_score4 = svm4.decision_function(X_train)\n",
    "\n",
    "false_pos_rate3, true_pos_rate3, _ = roc_curve(y_train, y_train_score3)\n",
    "roc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n",
    "\n",
    "false_pos_rate4, true_pos_rate4, _ = roc_curve(y_train, y_train_score4)\n",
    "roc_auc4 = auc(false_pos_rate4, true_pos_rate4)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
    "ax1.plot(false_pos_rate3, true_pos_rate3, label='SVM $\\gamma = 1$ ROC curve (area = %0.2f)' % roc_auc3, color='b')\n",
    "ax1.plot(false_pos_rate4, true_pos_rate4, label='SVM $\\gamma = 50$ ROC curve (area = %0.2f)' % roc_auc4, color='r')\n",
    "ax1.set_title('Training Data')\n",
    "\n",
    "y_test_score3 = svm3.decision_function(X_test)\n",
    "y_test_score4 = svm4.decision_function(X_test)\n",
    "\n",
    "false_pos_rate3, true_pos_rate3, _ = roc_curve(y_test, y_test_score3)\n",
    "roc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n",
    "\n",
    "false_pos_rate4, true_pos_rate4, _ = roc_curve(y_test, y_test_score4)\n",
    "roc_auc4 = auc(false_pos_rate4, true_pos_rate4)\n",
    "\n",
    "ax2.plot(false_pos_rate3, true_pos_rate3, label='SVM $\\gamma = 1$ ROC curve (area = %0.2f)' % roc_auc3, color='b')\n",
    "ax2.plot(false_pos_rate4, true_pos_rate4, label='SVM $\\gamma = 50$ ROC curve (area = %0.2f)' % roc_auc4, color='r')\n",
    "ax2.set_title('Test Data')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([-0.05, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multi-Class SVM\n",
    "+ Two different approaches for $K$ classes where $K > 2$. \n",
    "\n",
    "**One vs One** \n",
    "+ Also known as **all-pairs** approach.\n",
    "+ Constructs ${K \\choose 2}$ SVMs, each of which compares a pair of class.\n",
    "+ Test observations are assigned to the class which it was most frequently assigned. \n",
    "\n",
    "**One vs All** \n",
    "+ Constructs $K$ SVMs, each time comparing one of the $K$ class to the remaining $K-1$ classes. \n",
    "+ The test observation is belong to the class with the greatest distance from the hyperplane, i.e., $\\beta_{0k} + \\beta_{1k}x_1^* + \\beta_{2k}x_2^* +  \\ldots + \\beta_{pk}x_p^*$ is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Lab\n",
    "+ If the response is a factor containing more than two levels, then the ${\\tt svm()}$function will perform multi-class classification using the one-versus-one approach.\n",
    "+ We explore that setting here by generating a third class of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)\n",
    "XX = np.vstack([X, np.random.randn(50,2)])\n",
    "yy = np.hstack([y, np.repeat(0,50)])\n",
    "XX[yy ==0] = XX[yy == 0] +4\n",
    "\n",
    "plt.scatter(XX[:,0], XX[:,1], s=70, c=yy, cmap=plt.cm.prism)\n",
    "plt.xlabel('XX1')\n",
    "plt.ylabel('XX2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting an SVM to multiclass data uses identical syntax to fitting a simple two-class model:\n",
    "svm5 = SVC(C=1, kernel='rbf')\n",
    "svm5.fit(XX, yy)\n",
    "plot_svc(svm5, XX, yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Handwritten Letter Data\n",
    "\n",
    "+ We now examine [`Optical Recognition of Handwritten Digits Data Set`](http://archive.ics.uci.edu/ml/datasets/optical+recognition+of+handwritten+digits), which contains 5,620 samples of handwritten digits 0..9. \n",
    "+ You can use these links to download the [training data](http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra) and [test data](http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes), and then we'll load them into python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/optdigits.tra', header=None)\n",
    "y_train = X_train[64]\n",
    "X_train = X_train.drop(X_train.columns[64], axis=1)\n",
    "\n",
    "X_test = pd.read_csv('data/optdigits.tes', header=None)\n",
    "y_test = X_test[64]\n",
    "X_test = X_test.drop(X_test.columns[64], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the dimensions of this dataset:\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This data set consists of preprocessed images of handwriting samples gathered from 43 different people. \n",
    "+ Each image was converted into an 8x8 matrix (64 pixels), which was then flattened into a vector of 64 numeric values. \n",
    "+ The final column contains the class label for each digit.\n",
    "\n",
    "+ The training and test sets consist of 3,823 and 1,797 observations respectively. \n",
    "+ Let's see what one of these digits looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train.values[0].reshape(8,8), cmap=\"gray\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a linear kernel to see how we do:\n",
    "svc = SVC(kernel='linear', C=10)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Print a nice confusion matrix\n",
    "cm = confusion_matrix(y_train, svc.predict(X_train))\n",
    "cm_df = pd.DataFrame(cm.T, index=svc.classes_, columns=svc.classes_)\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We see that there are **no training errors**. \n",
    "+ This is not surprising, because the large number of variables relative to the number of observations implies that it is easy to find hyperplanes that fully separate the classes.\n",
    "+ We are most interested not in the support vector classifier’s performance on the training observations, but rather its performance on the test observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, svc.predict(X_test))\n",
    "print(pd.DataFrame(cm.T, index=svc.classes_, columns=svc.classes_))\n",
    "print(svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We see that using cost = 10 yields just 70 test set errors on this data. \n",
    "+ Now try using the  ${\\tt GridSearchCV()}$  function to select an optimal value for ${\\tt c}$. Consider values in the range 0.01 to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the optimal C parameter by cross-validation\n",
    "tuned_parameters = [{'C': [0.01, 0.1, 1, 5, 10, 100]}]\n",
    "clf = GridSearchCV(SVC(kernel='linear'), tuned_parameters, cv=10, scoring='accuracy')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2 = SVC(C=0.01, kernel='linear')\n",
    "svc2.fit(X_train, y_train)\n",
    "cm2 = confusion_matrix(y_test, svc2.predict(X_test))\n",
    "print(pd.DataFrame(cm2.T, index=svc2.classes_, columns=svc2.classes_))\n",
    "print(svc2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = np.arange(-2, 2, 1/20.)\n",
    "f = lambda x: 1 + 3*x\n",
    "plt.fill_between(section, f(section), y2=-2)\n",
    "plt.fill_between(section, f(section), y2=2, color='r')\n",
    "plt.text(.5, 0, \"> 0\", fontsize=16)\n",
    "plt.text(-1, 0, \"< 0\", fontsize=16)\n",
    "plt.ylim(-2, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = np.arange(-2, 2, 1/20.)\n",
    "f = lambda x: 1 - .5*x\n",
    "plt.fill_between(section, f(section), y2=-1)\n",
    "plt.fill_between(section, f(section), y2=3, color='r')\n",
    "plt.text(0, 2, \"< 0\", fontsize=16)\n",
    "plt.text(-1, 0, \"> 0\", fontsize=16)\n",
    "plt.ylim(-1, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = np.arange(-3, 1.05, 1/20.)\n",
    "f = lambda x: 2 + np.sqrt(4 - (1 + x)**2)\n",
    "g = lambda x: 2 - np.sqrt(4 - (1 + x)**2)\n",
    "plt.fill_between(np.arange(-5, 5), -10, 10)\n",
    "plt.fill_between(section, g(section), f(section), color='r')\n",
    "plt.text(-1, 2, \"< 4\", fontsize=16)\n",
    "plt.text(-1, -.5, \"> 4\", fontsize=16)\n",
    "plt.xlim(-4, 2)\n",
    "plt.ylim(-1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) \n",
    "* (0, 0) = blue\n",
    "* (-1, 1) = red  \n",
    "* (2, 2) = blue\n",
    "* (3, 8) = blue\n",
    "\n",
    "d) x1, x2, x1^2 and x^2 are linear in 4 dimensions. When it is projected down to 2 dimensions we get a circle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.array([[3, 4, -1], [2, 2, -1], [4, 4, -1], [1, 4, -1], [2, 1, 1], [4, 3, 1], [4, 1, 1]]), \n",
    "             columns=['x1', 'x2', 'y'])\n",
    "df['color'] = df['y'].map({1: 'r', -1:'b'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['x1'], df['x2'], color=df['color']);\n",
    "# maximum separating hyperplane is x1 - x2 - .5 = 0\n",
    "section = np.arange(-3, 8)\n",
    "f = lambda x: x - .5\n",
    "plt.plot(section, f(section))\n",
    "plt.plot(section, f(section) + .5, 'b--')\n",
    "plt.plot(section, f(section) - .5, 'b--')\n",
    "plt.ylim(0, 5)\n",
    "plt.xlim(0, 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Classification rule if x1 - x2 - .5 > 0 then red else blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) margin is distance from support vector to line.\n",
    "print(\"margin is\", .5 / np.sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) point (4, 1) is far from the hyperplane and changing it slightly will have no effect on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g Green line is non-optimal hyperplane. Still separates data but worse margin\n",
    "plt.scatter(df['x1'], df['x2'], color=df['color']);\n",
    "# maximum separating hyperplane is x1 - x2 - .5 = 0\n",
    "section = np.arange(-3, 8)\n",
    "f = lambda x: x - .5\n",
    "g = lambda x: 1.3 * x - 1.4\n",
    "plt.plot(section, f(section))\n",
    "plt.plot(section, g(section))\n",
    "plt.ylim(0, 5)\n",
    "plt.xlim(0, 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fake data where true boundary is x2 - x1 = 0\n",
    "X = np.random.rand(100, 2)\n",
    "d = 3 * (X[:, 0] - X[:, 1])\n",
    "n = np.random.randn(100)\n",
    "y = np.where(n > d, 0, 1)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercept_, clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "section = np.arange(0, 1, .01)\n",
    "\n",
    "beta0 = clf.intercept_\n",
    "beta1 = clf.coef_[0, 0]\n",
    "beta2 = clf.coef_[0, 1]\n",
    "\n",
    "f = lambda x: (beta0 + beta1 * x) / -beta2\n",
    "plt.plot(section, f(section))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn code found [here](http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html) to plot decision boundaries for svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = SVC(kernel='linear', C=C).fit(X, y)\n",
    "rbf_svc = SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "poly_svc = SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "xx, yy = np.meshgrid(np.arange(0, 1, h),\n",
    "                     np.arange(0, 1, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['Linear kernel',\n",
    "          'Polynomial Kernel',\n",
    "          'RBF kernel']\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for i, clf in enumerate((svc, poly_svc, rbf_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly actually does worse. Might need some tuning\n",
    "svc.score(X, y), rbf_svc.score(X, y), poly_svc.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(500,2) - .5\n",
    "y = (X[:, 0] ** 2 - X[:, 1] ** 2 > 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02 \n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(-.5, .5, h),\n",
    "                     np.arange(-.5, .5, h))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "Z = log_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.title(\"Logistic Regression with only 2 linear features\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.column_stack((X, X ** 2, X[:, 0] * X[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf.fit(X_new, y)\n",
    "\n",
    "h = .02 \n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(-.5, .5, h),\n",
    "                     np.arange(-.5, .5, h))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "Z = log_clf.predict(np.c_[xx.ravel(), yy.ravel(), xx.ravel()**2, yy.ravel()**2, xx.ravel() * yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.title(\"Logistic Regression with only 5 features: $X_1, X_2, X_1^2, X_2^2, X_1*X_2$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = .02  # step size in the mesh\n",
    "\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = SVC(kernel='linear', C=C).fit(X, y)\n",
    "poly_svc = SVC(kernel='poly', degree=2, C=1).fit(X, y)\n",
    "rbf_svc = SVC(kernel='rbf', gamma=.5, C=1).fit(X, y)\n",
    "\n",
    "# create a mesh to plot in\n",
    "xx, yy = np.meshgrid(np.arange(-.5, .5, h),\n",
    "                     np.arange(-.5, .5, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['Linear kernel',\n",
    "          'Polynomial Kernel',\n",
    "          'RBF kernel']\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "for i, clf in enumerate((svc, poly_svc, rbf_svc)):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Kernel fails here, must use polynomial or rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a linear decision boundary at x2 > .5\n",
    "X = np.random.rand(500, 2)\n",
    "y = (X[:, 1] > .5) * 1\n",
    "\n",
    "X_train = X[:250]\n",
    "X_test = X[250:]\n",
    "y_train = y[:250]\n",
    "y_test = y[250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Paired)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_C = np.logspace(-2,-1, 100)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for c in all_C:\n",
    "    svc = SVC(kernel='linear', C=c)\n",
    "    svc.fit(X_train, y_train)\n",
    "    train_scores.append(svc.score(X_train, y_train))\n",
    "    test_scores.append(svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_C, train_scores, label=\"Train\")\n",
    "plt.plot(all_C, test_scores, label=\"Test\")\n",
    "plt.title(\"Test Scores with different costs. Linear kernel\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think they worded this problem well. There needs to be a linearl separable data that has a wide enough decision boundary where points can be misclassified given new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv(\"data/auto.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto['y'] = (auto['mpg'] > auto['mpg'].median()) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat((pd.get_dummies(auto[['cylinders', 'origin', 'year']].astype(object)), \n",
    "               auto[['displacement', 'horsepower', 'weight', 'acceleration']],), axis=1)\n",
    "y = auto['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = auto.iloc[:, 1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.logspace(-4,1, 10):\n",
    "    clf = SVC(kernel='linear', C=c)\n",
    "    scores = cross_val_score(clf, X_new, y, n_jobs=-1, cv=5)\n",
    "    print(\"Linear SVM with c={} has test accuracy of {}\".format(round(c,4), round(scores.mean(), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in np.logspace(-2,3, 10):\n",
    "    gamma = .00001\n",
    "    clf = SVC(kernel='rbf', gamma=gamma, C=c)\n",
    "    scores = cross_val_score(clf, X_new, y, n_jobs=-1, cv=5)\n",
    "    print(\"RBF SVM with c={} and gamma = {} has test accuracy of {}\".format(round(c,4), gamma, round(scores.mean(), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial kernel not able to be solved in reasonable time"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

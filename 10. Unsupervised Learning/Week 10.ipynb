{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Unsupervised Learning\n",
    "+ Only have features, $X_1, X_2, \\ldots, X_p$. No response variable. \n",
    "+ Not interested in prediction.\n",
    "+ Will try and find interesting things in explanatory variables:\n",
    "    + Find an informative way to visualize data.\n",
    "    + Discover subgroups among the variables/observations.\n",
    "+ Principal components analysis: data visualization, data pre-processing.\n",
    "+ Clustering: discovering unknown subgroups in data. \n",
    "\n",
    "## Challenge of Unsupervised Learning\n",
    "+ Not always an exact goal.\n",
    "+ Part of an EDA.\n",
    "+ The results is hard to assess. No way to check the results.\n",
    "\n",
    "### Applications\n",
    "1. A cancer researcher might assay gene expression levels in 100 patients with breast cancer. He or she might then look for subgroups among the breast cancer samples, or among the genes, in order to obtain a better understanding of the disease.\n",
    "\n",
    "2. An online shopping site might try to identify groups of shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers.\n",
    "\n",
    "3. A search engine might choose what search results to display to a particular individual based on the click histories of other individuals with similar search patterns.\n",
    "\n",
    "## Principal Components Analysis\n",
    "+ Previously we discussed principal component regression where the original features were mapped to a smaller feature space that are then used as inputs into linear regression.\n",
    "+ The principal component directions are presented as directions in feature space along which the original data are highly variable.\n",
    "+ In PCA, these components is used in understanding the data.\n",
    "+ PCA can be used to visualize high dimensional data in 2 or 3 dimensions. \n",
    "\n",
    "## What are Principal Components\n",
    "+ PCA finds a low-dimensional representation of a data set that contains as much as possible of the variation.\n",
    "+ PCA seeks a small number of dimensions that are as interesting as possible. The concept of interesting is measured by the amount that the observations vary along each dimension.\n",
    "+ Each of the dimensions found by PCA is a linear combination of the p features.\n",
    "+ The first principal component is a weighted linear combination of all the original features \n",
    "$$Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\ldots + \\phi_{p1}X_p$$\n",
    "where the sum of the squared weights equals 1. These weights are the **loading factors**. \n",
    "+ The loading factors of the first principal component maximize the weighted sum of the features for each observation.\n",
    "+ $\\phi_1 = (\\phi_{11} \\; \\phi_{21} \\; \\cdots \\; \\phi_{p1} )^T$ is the loading vector which defines a direction in feature space along which the data vary the most.\n",
    "+ If we project the n data points onto this direction, the projected values are the principal component scores $z_{11}, \\ldots, z_{n1}$ themselves.\n",
    "+ The first PC can also be interpreted as the line closest to the data.\n",
    "+ The second principal component is uncorrelated with the first which makes it orthogonal to it.\n",
    "+ Very important to scale the data first - 0 mean, 1 std. The variances won't make sense otherwise.\n",
    "+ The principal components can be plot against each other in order to produce low-dimensional views of the data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: USArrests data set\n",
    "+ 50 states in the United States\n",
    "+ The number of arrests per 100, 000 residents for each of three crimes: **Assault**, **Murder**, and **Rape**. Also **UrbanPop** (the percent of the population in each state living in urban areas).\n",
    "+ PC score vectors length, $n=50$.\n",
    "+ PC loading vectors length, $p=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/USArrests.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Not surprisingly, the variables also have vastly different variances: the${\\tt UrbanPop}$ variable measures the percentage of the population in each state living in an urban area, which is not a comparable number to the number of crimes committeed in each state per 100,000 individuals. \n",
    "+ If we failed to scale the variables before performing PCA, then most of the principal components that we observed would be driven by the ${\\tt Assault}$ variable, since it has by far the largest mean and variance. \n",
    "+ Thus, it is important to standardize the variables to have mean zero and standard deviation 1 before performing PCA. \n",
    "+ We can do this using the ${\\tt scale()}$ function from ${\\tt sklearn}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X = pd.DataFrame(scale(df), index=df.index, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the ${\\tt PCA()}$ function from ${\\tt sklearn}$ to compute the loading vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.mean())\n",
    "print(X.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_loadings = pd.DataFrame(PCA().fit(X).components_.T, index=df.columns, columns=['V1', 'V2', 'V3', 'V4'])\n",
    "pca_loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We see that there are four distinct principal components. \n",
    "+ This is to be expected because there are in general ${\\tt min(n − 1, p)}$ informative principal components in a data set with $n$ observations and $p$ variables.\n",
    "+ Using the fit_transform() function, we can get the principal component scores of the original data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the PCA model and transform X to get the principal components\n",
    "pca = PCA()\n",
    "df_plot = pd.DataFrame(pca.fit_transform(X), columns=['PC1', 'PC2', 'PC3', 'PC4'], index=X.index)\n",
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a **biplot** of the first two principal components using our loading vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax1 = plt.subplots(figsize=(9,7))\n",
    "\n",
    "ax1.set_xlim(-3.5,3.5)\n",
    "ax1.set_ylim(-3.5,3.5)\n",
    "\n",
    "# Plot Principal Components 1 and 2\n",
    "for i in df_plot.index:\n",
    "    ax1.annotate(i, (df_plot.PC1.loc[i], df_plot.PC2.loc[i]), ha='center')\n",
    "\n",
    "# Plot reference lines\n",
    "ax1.hlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
    "ax1.vlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
    "\n",
    "ax1.set_xlabel('First Principal Component')\n",
    "ax1.set_ylabel('Second Principal Component')\n",
    "    \n",
    "# Plot Principal Component loading vectors, using a second y-axis.\n",
    "ax2 = ax1.twinx().twiny() \n",
    "\n",
    "ax2.set_ylim(-1,1)\n",
    "ax2.set_xlim(-1,1)\n",
    "ax2.set_xlabel('Principal Component loading vectors', color='red')\n",
    "\n",
    "# Plot labels for vectors. Variable 'a' is a small offset parameter to separate arrow tip and text.\n",
    "a = 1.07  \n",
    "for i in pca_loadings[['V1', 'V2']].index:\n",
    "    ax2.annotate(i, (pca_loadings.V1.loc[i]*a, pca_loadings.V2.loc[i]*a), color='red')\n",
    "\n",
    "# Plot vectors\n",
    "ax2.arrow(0,0,pca_loadings.V1[0], pca_loadings.V2[0])\n",
    "ax2.arrow(0,0,pca_loadings.V1[1], pca_loadings.V2[1])\n",
    "ax2.arrow(0,0,pca_loadings.V1[2], pca_loadings.V2[2])\n",
    "ax2.arrow(0,0,pca_loadings.V1[3], pca_loadings.V2[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The state names represent the scores for the first two principal components.\n",
    "+ The line indicate the first two principal component loading vectors. i.e. the loading for **Rape** on the 1st component is 0.54, and 2nd component -0.17.\n",
    "+ First loading vector places approximately equal weight on **Assault**, **Murder**, and **Rape**, with much less weight on **UrbanPop**. Hence this component roughly corresponds to a measure of overall rates of serious crimes.\n",
    "+ The second loading vector places most of its weight on **UrbanPop**. Hence, this component roughly corresponds to the level of urbanization of the state.\n",
    "+ This indicates that the crime-related variables are correlated with each other. States with high murder rates tend to have high assault and rape rates. And that the **UrbanPop** variable is less correlated with the other three.\n",
    "+ The loading vectors suggests that states with large positive scores on the first component, such as California, Nevada and Florida, have high crime rates. While states like North Dakota, with negative scores on the first component, have low crime rates.\n",
    "+ California also has a high score on the second component, indicating a high level of urbanization, while the opposite is true for states like Mississippi. \n",
    "+ States close to zero on both components, such as Indiana, have approximately average levels of both crime and urbanization.\n",
    "\n",
    "### Uniqueness of the Principal Components\n",
    "+ Each principal component loading vector is unique, up to a sign flip. \n",
    "+ This means that two different software packages will yield the same principal component loading vectors, although the signs of those loading vectors may differ (Refer to the example in the book)\n",
    "+ The signs may differ because each principal component loading vector specifies a direction in p-dimensional space: flipping the sign has no effect as the direction does not change.\n",
    "+ Similarly, the score vectors are unique up to a sign flip, since the variance of $Z$ is the same as the variance of $−Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of variance explained\n",
    "+ Each principal component explains some of the variance of the original data. \n",
    "+ We can find the proportion that each principal component explains by dividing each components variance by the total raw variance. \n",
    "+ Summing all the variances for each component equals 1.\n",
    "+ Examine a **scree plot** (for an elbow) to choose the number of principal components to use. Or can use cross validation to choose.\n",
    "+ The ${\\tt PCA()}$ function outputs the variance explained by of each principal component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the proportion of variance explained (PVE)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We see that the first principal component explains 62.0% of the variance in the data \n",
    "+ the next principal component explains 24.7% of the variance, and so forth. \n",
    "+ We can plot the PVE explained by each component as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot([1,2,3,4], pca.explained_variance_ratio_, '-o')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.xlim(0.75,4.25)\n",
    "plt.ylim(0,1.05)\n",
    "plt.xticks([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the function ${\\tt cumsum()}$, which computes the cumulative sum of the elements of a numeric vector, to plot the cumulative PVE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot([1,2,3,4], np.cumsum(pca.explained_variance_ratio_), '-s')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.xlim(0.75,4.25)\n",
    "plt.ylim(0,1.05)\n",
    "plt.xticks([1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: NCI60 Data Example\n",
    "+ 𝙽𝙲𝙸𝟼𝟶NCI60  cancer cell line microarray data, which consists of 6,830 gene expression measurements on 64 cancer cell lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/nci60_data.csv').drop('Unnamed: 0', axis=1)\n",
    "df2.columns = np.arange(df2.columns.size)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the labels to check our work later\n",
    "y = pd.read_csv('data/nci60_labs.csv', usecols=[1], skiprows=1, names=['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first perform PCA on the data after scaling the variables (genes) to\n",
    "have standard deviation one, although one could reasonably argue that it\n",
    "is better not to scale the genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "X = pd.DataFrame(scale(df2))\n",
    "X.shape\n",
    "\n",
    "# Fit the PCA model and transform X to get the principal components\n",
    "pca2 = PCA()\n",
    "df2_plot = pd.DataFrame(pca2.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the first few principal component score vectors, in order to\n",
    "visualize the data. The observations (cell lines) corresponding to a given\n",
    "cancer type will be plotted in the same color, so that we can see to what\n",
    "extent the observations within a cancer type are similar to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "\n",
    "color_idx = pd.factorize(y.type)[0]\n",
    "cmap = mpl.cm.hsv\n",
    "\n",
    "# Left plot\n",
    "ax1.scatter(df2_plot.iloc[:,0], df2_plot.iloc[:,1], c=color_idx, cmap=cmap, alpha=0.5, s=50)\n",
    "ax1.set_ylabel('Principal Component 2')\n",
    "\n",
    "# Right plot\n",
    "ax2.scatter(df2_plot.iloc[:,0], df2_plot.iloc[:,2], c=color_idx, cmap=cmap, alpha=0.5, s=50)\n",
    "ax2.set_ylabel('Principal Component 3')\n",
    "\n",
    "# Custom legend for the classes (y) since we do not create scatter plots per class (which could have their own labels).\n",
    "handles = []\n",
    "labels = pd.factorize(y.type.unique())\n",
    "norm = mpl.colors.Normalize(vmin=0.0, vmax=14.0)\n",
    "\n",
    "for i, v in zip(labels[0], labels[1]):\n",
    "    handles.append(mpl.patches.Patch(color=cmap(norm(i)), label=v, alpha=0.5))\n",
    "\n",
    "ax2.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# xlabel for both plots\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel('Principal Component 1') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ On the whole, cell lines corresponding to a single cancer type do tend to have similar values on the first few principal component score vectors. \n",
    "+ This indicates that cell lines from the same cancer type tend to have pretty similar gene expression levels.\n",
    "\n",
    "+ We can generate a summary of the proportion of variance explained (PVE) of the first few principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([df2_plot.iloc[:,:5].std(axis=0, ddof=0).to_numpy(),\n",
    "              pca2.explained_variance_ratio_[:5],\n",
    "              np.cumsum(pca2.explained_variance_ratio_[:5])],\n",
    "             index=['Standard Deviation', 'Proportion of Variance', 'Cumulative Proportion'],\n",
    "             columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ${\\tt plot()}$ function, we can also plot the variance explained by the\n",
    "first few principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_plot.iloc[:,:10].var(axis=0, ddof=0).plot(kind='bar', rot=0)\n",
    "plt.ylabel('Variances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is generally more informative to\n",
    "plot the PVE of each principal component (i.e. a **scree plot**) and the cumulative\n",
    "PVE of each principal component. This can be done with just a\n",
    "little tweaking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "# Left plot\n",
    "ax1.plot(pca2.explained_variance_ratio_, '-o')\n",
    "ax1.set_ylabel('Proportion of Variance Explained')\n",
    "ax1.set_ylim(ymin=-0.01)\n",
    "\n",
    "# Right plot\n",
    "ax2.plot(np.cumsum(pca2.explained_variance_ratio_), '-ro')\n",
    "ax2.set_ylabel('Cumulative Proportion of Variance Explained')\n",
    "ax2.set_ylim(ymax=1.05)\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel('Principal Component')\n",
    "    ax.set_xlim(-1,65)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We see that together, the first seven principal components explain around 40% of the variance in the data. \n",
    "+ This is not a huge amount of the variance. \n",
    "+ However, looking at the scree plot, we see that while each of the first seven principal components explain a substantial amount of variance, there is a marked decrease in the variance explained by further principal components. \n",
    "+ That is, there is an **elbow** in the plot after approximately the seventh principal component. \n",
    "+ This suggests that there may be little benefit to examining more than seven or so principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

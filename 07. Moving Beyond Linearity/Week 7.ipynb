{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Moving Beyond Linearity\n",
    "Linearity is almost always an approximation, need more flexible models.\n",
    "\n",
    "## Polynomial Regression\n",
    "+ Extend linear model with polynomial terms (e.g. $X^2, X^3, \\ldots$). \n",
    "+ It's still a linear (in parameter) model but can model non-linear data. \n",
    "+ Usually don't use polynomial terms higher than degree 3 or 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('images/pw52.png', width =700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw53.png', width =700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let explore how to generate the `Wage` dataset models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wage.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We first fit the polynomial regression model using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = PolynomialFeatures(1).fit_transform(df.age.values.reshape(-1,1))\n",
    "X2 = PolynomialFeatures(2).fit_transform(df.age.values.reshape(-1,1))\n",
    "X3 = PolynomialFeatures(3).fit_transform(df.age.values.reshape(-1,1))\n",
    "X4 = PolynomialFeatures(4).fit_transform(df.age.values.reshape(-1,1))\n",
    "X5 = PolynomialFeatures(5).fit_transform(df.age.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This syntax fits a linear model, using the `PolynomialFeatures()` function, in order to predict wage using up to a fourth-degree polynomial in `age`. \n",
    "+ The `PolynomialFeatures()` command allows us to avoid having to write out a long formula with powers\n",
    "of `age`. \n",
    "+ We can then fit our linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit2 = sm.GLS(df.wage, X4).fit()\n",
    "fit2.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Next, consider the task of predicting whether an individual earns more than \\$250,000 per year. \n",
    "+ First, create the appropriate response vector, and then fit a logistic model using the `GLM()` function from `statsmodels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create response matrix\n",
    "y = (df.wage > 250).map({False:0, True:1}).to_numpy()\n",
    "\n",
    "# Fit logistic model\n",
    "clf = sm.GLM(y, X4, family=sm.families.Binomial(sm.families.links.logit()))\n",
    "res = clf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create a grid of values for `age` at which we want predictions, and then call the generic `predict()` function for each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence of age values spanning the range\n",
    "age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)\n",
    "\n",
    "# Generate test data\n",
    "X_test = PolynomialFeatures(4).fit_transform(age_grid)\n",
    "\n",
    "# Predict the value of the generated ages\n",
    "pred1 = fit2.predict(X_test) # salary\n",
    "pred2 = res.predict(X_test)  # Pr(wage>250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Finally, plot the data and add the fit from the degree-4 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating plots\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (16,5))\n",
    "fig.suptitle('Degree-4 Polynomial', fontsize=14)\n",
    "\n",
    "# Scatter plot with polynomial regression line\n",
    "ax1.scatter(df.age, df.wage, facecolor='None', edgecolor='k', alpha=0.3)\n",
    "ax1.plot(age_grid, pred1, color = 'b')\n",
    "ax1.set_ylim(ymin=0)\n",
    "\n",
    "# Logistic regression showing Pr(wage>250) for the age range.\n",
    "ax2.plot(age_grid, pred2, color='b')\n",
    "\n",
    "# Rug plot showing the distribution of wage>250 in the training data.\n",
    "# 'True' on the top, 'False' on the bottom.\n",
    "ax2.scatter(df.age, y/5, s=30, c='grey', marker='|', alpha=0.7)\n",
    "\n",
    "ax2.set_ylim(-0.01,0.21)\n",
    "ax2.set_xlabel('age')\n",
    "ax2.set_ylabel('Pr(wage>250|age)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on a degree\n",
    "\n",
    "+ In performing a polynomial regression we must decide on the degree of the polynomial to use. One way to do this is by using hypothesis tests. \n",
    "+ Now fit models ranging from linear to a degree-5 polynomial and  determine the simplest model which is sufficient to explain the relationship between `wage` and `age`.\n",
    "+ Do this using the `anova_lm()` function, which performs an analysis of variance (ANOVA, using an F-test) in order to test the null hypothesis that a model $M_1$ is sufficient to explain the data against the  alternative hypothesis that a more complex model $M_2$ is required. \n",
    "+ In order to use the `anova_lm()` function, $M_1$ and $M_2$ must be **nested models**: the predictors in $M_1$ must be a subset of the predictors in $M_2$. \n",
    "+ In this case, we fit five different models and sequentially compare the simpler model to the more complex model \n",
    "\n",
    "(*Note:* you may get an *invalid value* Runtime Warning on the first model, because there is no \"simpler model\" to compare to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_1 = fit = sm.GLS(df.wage, X1).fit()\n",
    "fit_2 = fit = sm.GLS(df.wage, X2).fit()\n",
    "fit_3 = fit = sm.GLS(df.wage, X3).fit()\n",
    "fit_4 = fit = sm.GLS(df.wage, X4).fit()\n",
    "fit_5 = fit = sm.GLS(df.wage, X5).fit()\n",
    "\n",
    "print(sm.stats.anova_lm(fit_1, fit_2, fit_3, fit_4, fit_5, typ=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The $p$-value comparing the linear Model 1 to the quadratic Model 2 is essentially zero $(<10^{-32})$, indicating that a linear fit is not sufficient. \n",
    "+ Similarly the $p$-value comparing the quadratic Model 2 to the cubic Model 3 is very low (0.0017), so the quadratic fit is also insufficient. \n",
    "+ The $p$-value comparing the cubic and degree-4 polynomials, Model 3 and Model 4, is approximately 0.05 while the degree-5 polynomial Model 5 seems unnecessary because its $p$-value is 0.37. \n",
    "+ Hence, either a cubic or a quartic polynomial appear to provide a reasonable fit to the data, but lower- or higher-order models are not justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Functions\n",
    "+ Also known as piecewise constant regression.\n",
    "+ Cut $X$ into $K$ different regions and fit a constant to each region. \n",
    "$$y_i = \\beta_0 + \\beta_1C_1(x_i) + \\beta_2C_2(x_i) + \\ldots + \\beta_KC_K(x_i) + \\epsilon _i$$\n",
    "where\n",
    "$$\\begin{align*}\n",
    " C_0(X) &= I(X<c_1) \\\\\n",
    " C_1(X) &= I(c_1 \\leq X < c_2) \\\\\n",
    " & \\vdots \\\\\n",
    " C_{K-1}(X) &= I(c_{K-1} \\leq X < c_K) \\\\\n",
    " C_{K}(X) &= I(X \\geq c_K) \n",
    "\\end{align*}$$\n",
    "+ The model reduces to $\\hat{y} = \\beta_0 + \\beta_k$ where $k$ is the $k^\\textrm{th}$ region. \n",
    "+ $\\beta_0$ is just the estimate for y (the mean) in the region before the first cut point. \n",
    "+ Can use same approach for logistic regression to get a flat probability estimate for each region.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "+ In order to fit a step function, we use the `cut()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cut, bins = pd.cut(df.age, 4, retbins = True, right = True)\n",
    "df_cut.value_counts(sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Here `cut()` automatically picked the cutpoints at 33.5, 49, and 64.5 years of age. \n",
    "+ We could also have specified our own cutpoints directly. \n",
    "+ Now let's create a set of dummy variables for use in the regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_steps = pd.concat([df.age, df_cut, df.wage], keys = ['age','age_cuts','wage'], axis = 1)\n",
    "\n",
    "# Create dummy variables for the age groups\n",
    "df_steps_dummies = pd.get_dummies(df_steps['age_cuts'])\n",
    "\n",
    "# Statsmodels requires explicit adding of a constant (intercept)\n",
    "df_steps_dummies = sm.add_constant(df_steps_dummies)\n",
    "\n",
    "# Drop the (17.938, 33.5] category\n",
    "df_steps_dummies = df_steps_dummies.drop(df_steps_dummies.columns[1], axis = 1)\n",
    "\n",
    "df_steps_dummies.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Now to fit the models. \n",
    "+ We dropped the `age<33.5` category, so the intercept coefficient can be interpreted as the average salary for those under 33.5 years of age. \n",
    "+ The other coefficients can be interpreted as the average additional salary for those in the other age groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit3 = sm.GLM(df_steps.wage.to_numpy(), df_steps_dummies.astype(int).to_numpy()).fit()\n",
    "fit3.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The intercept coefficient of 94,160 can be interpreted as the average salary for those under 33.5 years of age. \n",
    "+ The other coefficients can be interpreted as the average additional salary for those in the other age groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the test data in the same bins as the training data.\n",
    "bin_mapping = np.digitize(age_grid.ravel(), bins)\n",
    "\n",
    "# Get dummies, drop first dummy category, add constant\n",
    "X_test2 = sm.add_constant(pd.get_dummies(bin_mapping).drop(1, axis = 1)).astype(int).to_numpy()\n",
    "\n",
    "# Predict the value of the generated ages using the linear model\n",
    "pred2 = fit3.predict(X_test2)\n",
    "\n",
    "# And the logistic model\n",
    "clf2 = sm.GLM(y, df_steps_dummies.astype(int).to_numpy(),\n",
    "              family=sm.families.Binomial(sm.families.links.logit()))\n",
    "res2 = clf2.fit()\n",
    "pred3 = res2.predict(X_test2)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12,5))\n",
    "fig.suptitle('Piecewise Constant', fontsize = 14)\n",
    "\n",
    "# Scatter plot with polynomial regression line\n",
    "ax1.scatter(df.age, df.wage, facecolor = 'None', edgecolor = 'k', alpha = 0.3)\n",
    "ax1.plot(age_grid, pred2, c = 'b')\n",
    "\n",
    "ax1.set_xlabel('age')\n",
    "ax1.set_ylabel('wage')\n",
    "ax1.set_ylim(ymin = 0)\n",
    "\n",
    "# Logistic regression showing Pr(wage>250) for the age range.\n",
    "ax2.plot(np.arange(df.age.min(), df.age.max()).reshape(-1,1), pred3, color = 'b')\n",
    "\n",
    "# Rug plot showing the distribution of wage>250 in the training data.\n",
    "# 'True' on the top, 'False' on the bottom.\n",
    "ax2.scatter(df.age, y/5, s = 30, c = 'grey', marker = '|', alpha = 0.7)\n",
    "\n",
    "ax2.set_ylim(-0.01, 0.21)\n",
    "ax2.set_xlabel('age')\n",
    "ax2.set_ylabel('Pr(wage>250|age)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis functions\n",
    "+ Polynomial terms and step function are both types of basis functions. \n",
    "+ A basis function, $b_k(X)$, is a function that transforms $X$. \n",
    "$$y_i = \\beta_0 + \\beta_1b_1(x_i) + \\beta_2b_2(x_i) + \\ldots + \\beta_Kb_K(x_i) + \\epsilon _i$$\n",
    "+ For polynomial regression, this is simply raising $X$ to a power and for step functions this is transforming $X$ into 0 or 1 based on whether $X$ is in a region or not (indicator variable). \n",
    "+ Wavelets and fourier series are also basis functions.\n",
    "\n",
    "## Regression Splines\n",
    "+ Combining piecewise constant regression and polynomial regression\n",
    "\n",
    "### Piecewise polynomials\n",
    "+ Fit separate low degree polynomials over different regions of $X$. \n",
    "+ It works by fitting a cubic regression model \n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2x_i^2 + \\beta_3 x_i^3 + \\epsilon_i$$\n",
    "where the coefficients differ in different parts of the range $X$.\n",
    "+ The place where the coefficients change are called knots. \n",
    "+ The polynomials are contrained so that they join smoothly at the knots.\n",
    "+ Example of piecewise cubic polynomial with as single knot at point $c$:\n",
    "$$y_i = \\begin{cases}\n",
    "\\beta_{01} + \\beta_{11} x_i + \\beta_{21}x_i^2 + \\beta_{31} x_i^3 + \\epsilon_i & \\textrm{if }x_i<c\\\\\n",
    "\\beta_{02} + \\beta_{12} x_i + \\beta_{22}x_i^2 + \\beta_{32} x_i^3 + \\epsilon_i & \\textrm{if }x_i\\geq c\n",
    "\\end{cases}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Splines\n",
    "+ Piecewise polynomials with constraints that the curves be continuous and smooth - meaning both first and second derivatives must match at the knot.\n",
    "+ Fitting a spline turns out to be surprisingly simpler than it seems. \n",
    "+ We don't have to fit a 3 degree polynomial for each region. \n",
    "+ By smartly choosing basis functions, we can use least squares to solve for all the coefficients. \n",
    "+ We use the truncated power basis function which is \n",
    "$$h(x, \\xi) = \\begin{cases}\n",
    "(x -\\xi)^3, & x > \\xi \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}$$ \n",
    "where $\\xi$ is a knot.\n",
    "\n",
    "+ The equation to send to least squares is \n",
    "$$\\hat{y} = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 +  b_1h(x, \\xi_1) + \\ldots + b_Kh(x, \\xi_K)$$ where we have $K$ truncated power transformations for a total of $K + 3$ predictors.\n",
    "\n",
    "### How to choose K?\n",
    "+ The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly.\n",
    "+ It is common to place knots in a uniform fashion. \n",
    "+ One way to do this is to specify the desired d.o.f., and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.\n",
    "+ Cross validation:\n",
    "    + remove a portion of the data (say 10 %),\n",
    "    + fit a spline with a certain number of knots to the remaining data, \n",
    "    + use the spline to make predictions for the held-out portion. \n",
    "    + repeat this multiple times until each observation has been left out once, and compute the overall cross-validated RSS. \n",
    "    + repeated for different numbers of knots K. \n",
    "    + the value of K giving the smallest RSS is chosen.\n",
    "    \n",
    "### Splines vs polynomial regression\n",
    "+ Splines generally do better. \n",
    "+ A complex fit can still be fit well with a 3 degree spline by placing more knots.\n",
    "+ It could take a very high degree polynomial to do the same and with worse variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw54.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing splines\n",
    "+ Finding a function that minimizes RSS but 'smooth'. \n",
    "+ Smoothness here is defined as having a relatively stable second derivative. \n",
    "+ We want to find the smoothing spline, $g$, that minimizes\n",
    "$$\\sum_{i=1}^n{(y_i - g(x_i))^2} + \\lambda \\int{g^{\\prime \\prime}(t)^2 dt}$$\n",
    "+ The first term is a *loss function* that encourages g to fit the data well. \n",
    "+ The second term is a penalty term that penalize the variability in $g$.\n",
    "+ Larger tuning parameter,$\\lambda$, will make $g$ smoother.\n",
    "+ The function that minimizes this error is a natural cubic spline with knots at each unique value of x but with shrunken parameter estimates due to the penalty term. \n",
    "+ The tuning (smoothing) parameter is very important to control variance. Choose smoothing parameter with CV.\n",
    "+ Effective degrees of freedom, $df_\\lambda$, is a measure of the flexibility of the smoothing spline (higher $df_\\lambda$ -> more flexible(low bias, high variance)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the data\n",
    "df = pd.read_csv('data/wage.csv')\n",
    "\n",
    "# Generate a sequence of age values spanning the range\n",
    "age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In order to fit regression splines in python, we use the ${\\tt dmatrix}$ module from the ${\\tt patsy}$ library. \n",
    "+ Regression splines can be fit by constructing an appropriate matrix of basis functions. \n",
    "+ The ${\\tt bs()}$ function generates the entire matrix of basis functions for splines with the specified set of knots.  \n",
    "+ Fitting ${\\tt wage}$ to ${\\tt age}$ using a regression spline is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting  wage  to  age  using a regression spline \n",
    "\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Specifying 3 knots\n",
    "transformed_x1 = dmatrix(\"bs(df.age, knots=(25,40,60), degree=3, include_intercept=False)\",\n",
    "                        {\"df.age\": df.age}, return_type='dataframe')\n",
    "\n",
    "# Build a regular linear model from the splines\n",
    "fit1 = sm.GLM(df.wage, transformed_x1).fit()\n",
    "fit1.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw57.png', width =600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Here we have prespecified knots at ages 25, 40, and 60. \n",
    "+ This produces a spline with six basis functions. (Recall that a cubic spline with three knots has seven degrees of freedom; these degrees of freedom are used up by an intercept, plus six basis functions.) \n",
    "+ We could also use the ${\\tt df}$ option to produce a spline with knots at uniform quantiles of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the df  option to produce a spline with knots at uniform quantiles of the data:\n",
    "# Specifying 6 degrees of freedom \n",
    "\n",
    "transformed_x2 = dmatrix(\"bs(df.age, df=6, include_intercept=False)\",\n",
    "                        {\"df.age\": df.age}, return_type='dataframe')\n",
    "fit2 = sm.GLM(df.wage, transformed_x2).fit()\n",
    "fit2.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_x2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In this case python chooses knots which correspond to the 25th, 50th, and 75th percentiles of ${\\tt age}$. \n",
    "+ The function ${\\tt bs()}$ also has a ${\\tt degree}$ argument, so we can fit splines of any degree, rather than the\n",
    "default degree of 3 (which yields a cubic spline).\n",
    "\n",
    "+ In order to instead fit a natural spline, we use the ${\\tt cr()}$ function. \n",
    "+ Here we fit a natural spline with four degrees of freedom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fit a natural spline, we use the  ðšŒðš›() function. \n",
    "# Specifying 4 degrees of freedom\n",
    "\n",
    "transformed_x3 = dmatrix(\"cr(df.age, df=4)\", {\"df.age\": df.age}, return_type='dataframe')\n",
    "fit3 = sm.GLM(df.wage, transformed_x3).fit()\n",
    "fit3.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ As with the ${\\tt bs()}$ function, we could instead specify the knots directly using the ${\\tt knots}$ option.\n",
    "\n",
    "+ Let's see how these three models stack up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sequence of age values spanning the range\n",
    "age_grid = np.arange(df.age.min(), df.age.max()).reshape(-1,1)\n",
    "\n",
    "# Make some predictions\n",
    "pred1 = fit1.predict(dmatrix(\"bs(age_grid, knots=(25,40,60), include_intercept=False)\",\n",
    "                             {\"age_grid\": age_grid}, return_type='dataframe'))\n",
    "pred2 = fit2.predict(dmatrix(\"bs(age_grid, df=6, include_intercept=False)\",\n",
    "                             {\"age_grid\": age_grid}, return_type='dataframe'))\n",
    "pred3 = fit3.predict(dmatrix(\"cr(age_grid, df=4)\", {\"age_grid\": age_grid}, return_type='dataframe'))\n",
    "\n",
    "# Plot the splines and error bands\n",
    "plt.scatter(df.age, df.wage, facecolor='None', edgecolor='k', alpha=0.1)\n",
    "plt.plot(age_grid, pred1, color='b', label='Specifying three knots')\n",
    "plt.plot(age_grid, pred2, color='r', label='Specifying df=6')\n",
    "plt.plot(age_grid, pred3, color='g', label='Natural spline df=4')\n",
    "[plt.vlines(i , 0, 350, linestyles='dashed', lw=2, colors='b') for i in [25,40,60]]\n",
    "plt.legend()\n",
    "plt.xlim(15,85)\n",
    "plt.ylim(0,350)\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('wage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Regression\n",
    "+ Fits a new regression line to each point by using the nearest neighbors of that point. \n",
    "+ It uses weighted least squares, weighing points at the boundary and beyond 0 and points in the boundary a decreasing function of its distance to the point. \n",
    "+ Usually, small degree polynomials are fit to these local points. \n",
    "+ Need to choose weight function and span, $s$ of points. \n",
    "+ Larger span of points the smoother function you will get.\n",
    "1. Gather the fraction, $s=k/n$ of training points whose $x_i$ are closest to $x_0$.\n",
    "2. Assign a weight $K_{i0} = K(x_i, x_0)$ to each point in the neighbourhood. All but these $k$ nearest neighbors get weight zero.\n",
    "3. Fit a weighted least squares regression of the $y_i$ on the $x_i$, by finding $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize\n",
    "$$\\sum_{i=1}^n{K_{i0}(y_i - \\beta_0 - \\beta_1x_i)^2}$$\n",
    "4. The fitted value at $x_0$ is $$\\hat{f}(x_0) = \\hat{\\beta}_0 + \\hat{\\beta}_1x_0$$\n",
    "+ Can even do local regression with pairs or more of variables but because of the curse of dimensionality, there might not be enough neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw55.png', width =700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw56.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Additive Models\n",
    "+ All the previous models all relate to single variable predictions. \n",
    "+ GAMs simply add different linear models above (like the ones above) for different variables in the model, allowing for multivariate regression/classification. \n",
    "+ Each variable gets its own model and is added together. \n",
    "+ Each own model is a building block for a GAM.\n",
    "\n",
    "### GAM for regression\n",
    "$$y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\ldots + \\beta_px_{ip} + \\epsilon_i$$\n",
    "+ Replace each linear component, $\\beta_j x_{ij}$, with a smooth nonlinear function, $f_j (x_{ij})$,\n",
    "$$ y_i = \\beta_0 + f_1(x_{i1}) + \\ldots + f_p(x_{ip}) + \\epsilon_i$$\n",
    "+ GAM can use the previous methods as building blocks for fitting an additive model.\n",
    "+ The main limitation of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed.\n",
    "+ However, we can manually add interaction terms by including additional predictors of the form $X_j \\times X_k$. \n",
    "+ Or, we can add low-dimensional interaction functions of the form $f_{jk}(X_j,X_k)$ into the model using two-dimensional smoothers such as local regression, or two-dimensional splines.\n",
    "+ The same technique can also be used for classification problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw58.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7.8.1\n",
    "Recreating plot 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage = pd.read_csv(\"data/wage.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn to get regression coefficients\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=4, include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wage[['age']]\n",
    "y = wage['wage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(poly.fit_transform(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients are the same as in ISLR\n",
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard error in  Scikit-learn\n",
    "Sklearn doesn't supply the standard error so you'll have to write the formula yourself or use statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4)', data=wage).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.bse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval for the mean\n",
    "There are different confidence intervals for the mean (the regression line) and prediction. Prediction intervals are going to be much wider. The regression line will not wiggle around so much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st, data, ss2 = summary_table(results, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedvalues = data[:,2]\n",
    "predict_mean_se  = data[:,3]\n",
    "predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(X.values.flatten())\n",
    "x_o = X.values.flatten()[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_o, y[order])\n",
    "plt.plot(x_o, fittedvalues[order], 'r', lw=2)\n",
    "plt.plot(x_o, predict_mean_ci_low[order], 'r--', lw=2)\n",
    "plt.plot(x_o, predict_mean_ci_upp[order], 'r--', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features are necessary\n",
    "smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4)', data=wage).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Anova to test wheter each additional polynomial term is significant\n",
    "Models must be nested here, meaning that mod2 must be a superset of mod1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.api import anova_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = smf.ols('wage ~ age', data=wage).fit()\n",
    "mod2 = smf.ols('wage ~ age + np.power(age, 2)', data=wage).fit()\n",
    "mod3 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3)', data=wage).fit()\n",
    "mod4 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4)', data=wage).fit()\n",
    "mod5 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4) + np.power(age, 5)', data=wage).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as ISLR\n",
    "# polynomial terms 4 and 5 are not needed. p > .05\n",
    "anova_lm(mod1, mod2, mod3, mod4, mod5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "Prediciton of greater than 250k in income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage['wage_250'] = (wage['wage'] > 250) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.logit('wage_250 ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4)', data=wage).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.exp(results.fittedvalues)\n",
    "y = wage['wage_250'].values\n",
    "x = X['age'].values\n",
    "x_mean = x.mean()\n",
    "n = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy = np.sqrt(np.sum((y - y_hat)**2) / (n - 2))\n",
    "sx = np.sum((x - x_mean) ** 2) / n\n",
    "x_s = (x - x_mean) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = np.sum(x ** 2) - (x.sum() ** 2) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = sy * np.sqrt(1/n + x_s / x_s.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(x)\n",
    "x_o = x[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(x_o, y[order])\n",
    "plt.plot(x_o, y_hat[order], 'r', lw=2)\n",
    "plt.plot(x_o, y_hat[order] + 2 * err[order], 'r--', lw=2)\n",
    "plt.plot(x_o, y_hat[order] - 2 * err[order], 'r--', lw=2)\n",
    "plt.ylim(0, .07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step function as in 7.2\n",
    "use pd.cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('wage ~ pd.cut(age, 4)', data=wage).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = wage['wage'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sort = x[order]\n",
    "y_sort = y[order]\n",
    "t = np.array([25, 40, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = si.LSQUnivariateSpline(x_sort, y_sort, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl(x_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x_sort, y_sort, 'ro', ms=5)\n",
    "plt.plot(x_sort, spl(x_sort), 'g-', lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General additive models for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pygam --user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data contains 569 observations and 30 features. The target variable in this case is whether the tumor of malignant or benign, and the features are several measurements of the tumor. For showcasing purposes, we keep the first 6 features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd        \n",
    "from pygam import LogisticGAM\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "#load the breast cancer data set\n",
    "data = load_breast_cancer()\n",
    "\n",
    "#keep first 6 features only\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)[['mean radius', 'mean texture', 'mean perimeter', 'mean area','mean smoothness', 'mean compactness']]\n",
    "target_df = pd.Series(data.target)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a classification problem, make sure to use pyGamâ€™s LogisticGAM() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['mean radius', 'mean texture', 'mean perimeter', 'mean area','mean smoothness', 'mean compactness']]\n",
    "y = target_df\n",
    "\n",
    "#Fit a model with the default parameters\n",
    "gam = LogisticGAM().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam.accuracy(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (28, 8)\n",
    "fig, axs = plt.subplots(1, len(data.feature_names[0:6]))\n",
    "titles = data.feature_names\n",
    "for i, ax in enumerate(axs):\n",
    "    XX = gam.generate_X_grid(term=i)\n",
    "    pdep, confi = gam.partial_dependence(term=i, width=.95)\n",
    "    ax.plot(XX[:, i], pdep)\n",
    "    ax.plot(XX[:, i], confi[:, 0], c='grey', ls='--')\n",
    "    ax.plot(XX[:, i], confi[:, 1], c='grey', ls='--')\n",
    "    ax.set_title(titles[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAM for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LinearGAM, s, f\n",
    "from pygam.datasets import wage\n",
    "\n",
    "X, y = wage(return_X_y=True)\n",
    "\n",
    "## model\n",
    "gam = LinearGAM(s(0) + s(1) + f(2))\n",
    "gam.gridsearch(X, y)\n",
    "\n",
    "\n",
    "## plotting\n",
    "plt.figure();\n",
    "fig, axs = plt.subplots(1,3);\n",
    "\n",
    "titles = ['year', 'age', 'education']\n",
    "for i, ax in enumerate(axs):\n",
    "    XX = gam.generate_X_grid(term=i)\n",
    "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
    "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')\n",
    "    if i == 0:\n",
    "        ax.set_ylim(-30,30)\n",
    "    ax.set_title(titles[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on this: https://pygam.readthedocs.io/en/latest/notebooks/tour_of_pygam.html#Functional-Form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Regression\n",
    "\n",
    "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wage = pd.read_csv(\"data/wage.csv\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train , test = train_test_split(wage, test_size = 0.3)\n",
    "\n",
    "X_train = train[['age']]\n",
    "y_train = train['wage']\n",
    "X_test = test[['age']]\n",
    "y_test = test['wage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn=KNeighborsRegressor(n_neighbors=9)\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn=knn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train,y_train,color=\"blue\")\n",
    "plt.scatter(X_test,knn.predict(X_test),color=\"red\")\n",
    "plt.title(\"Wage Prediction\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Wage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse_val = [] #to store rmse values for different k\n",
    "for K in range(100):\n",
    "    K = K+1\n",
    "    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
    "    model.fit(X_train, y_train) #fit the model\n",
    "    pred=model.predict(X_test) #make prediction on test set\n",
    "    error = sqrt(mean_squared_error(y_test,pred)) #calculate rmse\n",
    "    rmse_val.append(error) #store rmse values\n",
    "    print(\"RMSE value for k= \" , K , \"is:\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the rmse values against k values\n",
    "curve = pd.DataFrame(rmse_val) #elbow curve\n",
    "curve.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"n_neighbors\":list(range(1,100))}\n",
    "knn = neighbors.KNeighborsRegressor()\n",
    "model = GridSearchCV(knn, params, cv=5)\n",
    "model.fit(X_train,y_train)\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, neighbors, linear_model\n",
    "\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "X_digits = X_digits / X_digits.max()\n",
    "\n",
    "n_samples = len(X_digits)\n",
    "\n",
    "X_train = X_digits[:int(.9 * n_samples)]\n",
    "y_train = y_digits[:int(.9 * n_samples)]\n",
    "X_test = X_digits[int(.9 * n_samples):]\n",
    "y_test = y_digits[int(.9 * n_samples):]\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "logistic = linear_model.LogisticRegression(max_iter=1000)\n",
    "\n",
    "print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Cubic Spline regression\n",
    "a) Since x < $\\xi$ then $a_1 = \\beta_0$ and $b_1 = \\beta_1$ and $c_1 = \\beta_2$ and $d_1 = \\beta_3$\n",
    "\n",
    "b) Must expand expression and group like polynomial terms. $a_1 = \\beta_0 - \\beta_4\\xi^3$ and $b_1 = \\beta_1 + \\beta_4\\xi^2$ and $c_1 = \\beta_2 - 3\\beta_4\\xi$ and $d_1 = \\beta_3 + \\beta_4$\n",
    "\n",
    "c) when $x=\\xi$ the spline term equals 0 for both equations and thus they are equal\n",
    "\n",
    "d, e) If they are equal then their derivatives must also be equal\n",
    "\n",
    "# 2\n",
    "a) g = 0  \n",
    "b) g = mean(y)  \n",
    "c) g = linear regression with 2 parameters - slope and intercept  \n",
    "d) g = cubic term in regression with 3 parameters  \n",
    "e) g = very high dimensional function that gives nearly 0 training error  \n",
    "\n",
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1 + x + -2 * (x - 1) ** 2 * (x >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_1 = (0 <= x) & (x <= 2)\n",
    "b1_2 = (1 <= x) & (x <= 2)\n",
    "b2_1 = (3 <= x) & (x <= 4)\n",
    "b2_2 = (4 < x) & (x <= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1 + b1_1 - (x - 1) * b1_2 + (x - 3) * b2_1 + b2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5\n",
    "a) g2 will have smaller training error, since it is allowing more flexibility, can have up to a cubic model. g1 will be limited to a quadratic model as $\\lambda$ approaches infinity\n",
    "\n",
    "b) Can't tell which model will have smaller test error this depends on the 'true' relationship between x and y.\n",
    "\n",
    "c) g1 and g2 will be the same model if there is no penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 11)\n",
    "X = wage[['age']]\n",
    "y = wage['wage']\n",
    "final_scores = []\n",
    "for degree in degrees:\n",
    "    polynomial_features = PolynomialFeatures(degree=degree,\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "\n",
    "    scores = model_selection.cross_val_score(pipeline,\n",
    "                                            X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "    final_scores.append(-np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree 3 chosen through 10-fold CV\n",
    "plt.plot(degrees, final_scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to anova: already done above. More evidence that 4th and 5th degree polynomial are not needed\n",
    "mod1 = smf.ols('wage ~ age', data=wage).fit()\n",
    "mod2 = smf.ols('wage ~ age + np.power(age, 2)', data=wage).fit()\n",
    "mod3 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3)', data=wage).fit()\n",
    "mod4 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4)', data=wage).fit()\n",
    "mod5 = smf.ols('wage ~ age + np.power(age, 2) + np.power(age, 3) + np.power(age, 4) + np.power(age, 5)', data=wage).fit()\n",
    "anova_lm(mod1, mod2, mod3, mod4, mod5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "linear_regression.fit(polynomial_features.fit_transform(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(X.values.min(), X.values.max(), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(x, linear_regression.predict(polynomial_features.fit_transform(x.reshape(-1, 1))), c='r', lw=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts = range(1, 41)\n",
    "X = wage[['age']]\n",
    "y = wage['wage']\n",
    "final_scores = []\n",
    "for cut in cuts:\n",
    "    X_new = pd.get_dummies(pd.cut(X['age'], cut)).values\n",
    "    \n",
    "    linear_regression = LinearRegression(fit_intercept=False)\n",
    "\n",
    "    scores = model_selection.cross_val_score(linear_regression, X_new, y, cv=10, scoring='neg_mean_squared_error')\n",
    "    final_scores.append(-np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like error stops getting better after 7 cuts\n",
    "plt.plot(cuts, final_scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.get_dummies(pd.cut(X['age'], 7)).values\n",
    "linear_regression = LinearRegression(fit_intercept=False)\n",
    "linear_regression.fit(X_new, y)\n",
    "plt.scatter(X, y)\n",
    "order = np.argsort(X['age'])\n",
    "plt.plot(X['age'].values[order], linear_regression.predict(X_new[order]), c='r', lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage = pd.read_csv('data/wage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage[['maritl', 'jobclass']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(wage[['maritl', 'jobclass']], drop_first=False)\n",
    "y = wage['wage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression(fit_intercept=True)\n",
    "linear_regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_orig = smf.OLS(y, X).fit()\n",
    "results_orig.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage[(wage['jobclass'] == '2. Information') & (wage['maritl'] == '3. Widowed')]['wage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage[wage['maritl'] == '1. Never Married']['wage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage[wage['jobclass'] == '1. Industrial']['wage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage[wage['jobclass'] == '2. Information']['wage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage['jobclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage[(wage['jobclass'] == '2. Information') & (wage['maritl'] == '3. Widowed')]['wage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "27.6 + 82.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(wage['maritl'] + ' ' + wage['jobclass'])\n",
    "y = wage['wage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.OLS(y, X).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage[(wage['jobclass'] == '2. Information') & (wage['maritl'] == '3. Widowed')]['wage'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.predict([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_orig.predict([0, 0, 0, 1, 0, 1, 0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

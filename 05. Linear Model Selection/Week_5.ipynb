{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Linear Model Selection and Regularization\n",
    "Linear models still can do astonishingly well compared to non-linear models. This chapter will explore other types of fitting besides least squares because they can give better prediction accuracy and interpretability.\n",
    "\n",
    "## Classes of alternatives to least squares\n",
    "* Subset Selection - Choose a subset of the predictors\n",
    "* Shrinkage (regularization) - Fit all predictors but limit their size. Coefficients can go to 0.\n",
    "* Dimension Reduction - project the predictors into a smaller subspace\n",
    "\n",
    "## Best Subset Selection\n",
    "Fit all possible models $2^p$ and take the best model using cross-validated\n",
    "prediction error, Cp, AIC, BIC, or adjusted $R^2$. This can be impossible with large enough p\n",
    "\n",
    "## Stepwise Selection - Forward, Backward and both\n",
    "Because of computational limitation a simpler method of adding or subtracting the best predictor to the current model is employed.\n",
    "\n",
    "### Forward Selection\n",
    "Start with an empty model and choose on predictor to add to the model based on best adjusted $R^2$ or other similar metric. Continue adding variables until no improvement in adjusted $R^2$. \n",
    "\n",
    "A variation to this is to retain each model at each step and use $R^2$ (not adjusted) as the metric. This will build p models. Then use cross validation with $R^2$ to pick the best model of those p models built from forward selection.\n",
    "\n",
    "### Backward Selection\n",
    "Similar to forward selection but start with all predictors in model and remove one at a time until adjusted $R^2$ is maximized or alternatively, find p models with $R^2$ and then use cross validation to pick best of the p models.\n",
    "\n",
    "## Both\n",
    "At each step, consider both adding or subtracting a variable in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Best subset\n",
    "Here we apply the best subset selection approach to the Hitters data. We\n",
    "wish to predict a baseball player’s Salary on the basis of various statistics\n",
    "associated with performance in the previous year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = \\beta_0$ find the $R_{adj}=0.2$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = \\beta_0 + \\beta_1 x_1$ $R_{adj}=0.2$\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_2$ $R_{adj}=0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = \\beta_0 + \\beta_1 x_2 + \\beta_2 x_1$ $R_{adj}=0.4$\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_2 + \\beta_2 x_3$ $R_{adj}=0.6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\vdots$\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_2 + \\beta_2 x_3 + \\beta_3 x_5$ $R_{adj}=0.8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backward\n",
    "$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_5 x_5$ $R_{adj}=0.8$\n",
    "\n",
    "$y = \\beta_0 +  \\beta_2 x_2 + \\cdots + \\beta_5 x_5$ $R_{adj}=0.85$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F/B selection\n",
    "$y = \\beta_0 $ $R_{adj}=0.2$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_2 + \\beta_2 x_3 + \\beta_3 x_5$ $R_{adj}=0.8$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 x_2 + \\beta_3 x_5$ $R_{adj}=0.85$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hitters_df = pd.read_csv('data/hitters.csv')\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we note that the `Salary` variable is missing for some of the\n",
    "players. The `isnull()` function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a `TRUE` value\n",
    "for any elements that are missing, and a `FALSE` value for non-missing elements.\n",
    "The `sum()` function can then be used to count all of the missing elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of null values:\", hitters_df[\"Salary\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensions of the original Hitters data (322 rows x 20 columns)\n",
    "print(\"Dimensions of original data:\", hitters_df.shape)\n",
    "\n",
    "# Drop any rows the contain missing values, along with the player names\n",
    "hitters_df_clean = hitters_df.dropna().drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Print the dimensions of the modified Hitters data (263 rows x 20 columns)\n",
    "print(\"Dimensions of modified data:\", hitters_df_clean.shape)\n",
    "\n",
    "# One last check: should return 0\n",
    "print(\"Number of null values:\", hitters_df_clean[\"Salary\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(hitters_df_clean[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "y = hitters_df_clean.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = hitters_df_clean.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform best subset selection by identifying the best model that contains a given number of predictors, where **best** is quantified using RSS. We'll define a helper function to outputs the best set of variables for\n",
    "each model size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y,X[list(feature_set)].astype(float))\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBest(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a `DataFrame` containing the best model that we generated, along with some extra information about the model. Now we want to call that function for each number of predictors $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could take quite awhile to complete...\n",
    "\n",
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,8):\n",
    "    models_best.loc[i] = getBest(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have one big DataFrame that contains the best models we've generated along with their RSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to access the details of each model, no problem! We can get a full rundown of a single model using the `summary()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_best.loc[7, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output indicates that the best two-variable model contains only Hits and CRBI. To save time, we only generated results up to the best 7-variable model. You can use the functions we defined above to explore as many variables as are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best 19-variable model (there's actually only one)\n",
    "print(getBest(17)[\"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than letting the results of our call to the `summary()` function print to the screen, we can access just the parts we need using the model's attributes. For example, if we want the $R^2$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_best.loc[2, \"model\"].rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the verbose output we get when we print the summary to the screen, fitting the `OLM` also produced many other useful statistics such as adjusted $R^2$, AIC, and BIC. We can examine these to try to select the best overall model. Let's start by looking at $R^2$ across all our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the second element from each row ('model') and pulls out its rsquared attribute\n",
    "models_best.apply(lambda row: row[1].rsquared, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the $R^2$ statistic increases monotonically as more\n",
    "variables are included.\n",
    "\n",
    "Plotting RSS, adjusted $R^2$, AIC, and BIC for all of the models at once will\n",
    "help us decide which model to select. Note the `type=\"l\"` option tells `R` to\n",
    "connect the plotted points with lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "# Set up a 2x2 grid so we can look at 4 plots at once\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "plt.plot(models_best[\"RSS\"])\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RSS')\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "rsquared_adj = models_best.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rsquared_adj)\n",
    "plt.plot(rsquared_adj.argmax()+1, rsquared_adj.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('adjusted rsquared')\n",
    "\n",
    "# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "aic = models_best.apply(lambda row: row[1].aic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin()+1, aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "bic = models_best.apply(lambda row: row[1].bic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin()+1, bic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquared_adj.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsquared_adj.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the second step of our selection process, we narrowed the field down to just one model on any $k<=p$ predictors. We see that according to BIC, the best performer is the model with 6 variables. According to AIC and adjusted $R^2$ something a bit more complex might be better. Again, no one measure is going to give us an entirely accurate picture... but they all agree that a model with 5 or fewer predictors is insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(predictors):\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X.columns if p not in predictors]\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p]))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_fwd = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,len(X.columns)+1):    \n",
    "    models_fwd.loc[i] = forward(predictors)\n",
    "    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_fwd.loc[1, \"model\"].summary())\n",
    "print(models_fwd.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using forward stepwise selection, the best one-variable\n",
    "model contains only `Hits`, and the best two-variable model additionally\n",
    "includes `CRBI`. Let's see how the models stack up against best subset selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_best.loc[6, \"model\"].summary())\n",
    "print(models_fwd.loc[6, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(predictors):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)-1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_bwd = pd.DataFrame(columns=[\"RSS\", \"model\"], index = range(1,len(X.columns)))\n",
    "\n",
    "tic = time.time()\n",
    "predictors = X.columns\n",
    "\n",
    "while(len(predictors) > 1):  \n",
    "    models_bwd.loc[len(predictors)-1] = backward(predictors)\n",
    "    predictors = models_bwd.loc[len(predictors)-1][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------\")\n",
    "print(\"Best Subset:\")\n",
    "print(\"------------\")\n",
    "print(models_best.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------\")\n",
    "print(\"Foward Selection:\")\n",
    "print(\"-----------------\")\n",
    "print(models_fwd.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------------\")\n",
    "print(\"Backward Selection:\")\n",
    "print(\"-------------------\")\n",
    "print(models_bwd.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Optimal Model \n",
    "## Adjusting Training Statistics or Using Cross Validation\n",
    "As learned in Chapter 5, cross validation is an extremely good tool at giving us insight to how well the model will be used on unseen data (test data).\n",
    "\n",
    "But alternatively to cross validation, we can punish the training error statistics so in theory they can give us insight on what the test error will be. There have been several statistics developed to give us insight as to what the model will do for unseen errors.\n",
    "\n",
    "The 4 most popular are AIC, BIC, Mallows Cp and Adjusted R^2. AIC, BIC and Cp all have similar formulas that inflate the error for more predictors and a higher estimated variance. Adjusted R squared lowers the R^2 by each additional predictor in the model. All 4 of these statistics are 'classical' model selectors and were commonly used.\n",
    "\n",
    "Cross validation can be computationally intense but with modern computation we can build lots of models and evaluate them easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Model selection using the Validation Set Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hitters.csv')\n",
    "\n",
    "# Drop any rows the contain missing values, along with the player names\n",
    "df = df.dropna().drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Get dummy variables\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "# Extract independent variable\n",
    "y = pd.DataFrame(df.Salary)\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In order for the validation set approach to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of model-fitting — including variable selection. \n",
    "+ Therefore, the determination of which model of a given size is best must be made using only the training observations. \n",
    "+ This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.\n",
    "\n",
    "+ In order to use the validation set approach, we begin by splitting the observations into a training set and a test set. \n",
    "+ We do this by creating a random vector, train, of elements equal to TRUE if the corresponding observation is in the training set, and FALSE otherwise. \n",
    "+ The vector test is TRUE if the observation is in the test set, and a FALSE otherwise. \n",
    "+ Note the np.invert() in the command to create test causes TRUEs to be switched to FALSEs and vice versa.\n",
    "+ We also set a random seed so that the user will obtain the same training set/test set split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=12)\n",
    "train = np.random.choice([True, False], size = len(y), replace = True)\n",
    "test = np.invert(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We'll define our helper function to outputs the best set of variables for each model size like we did previously. \n",
    "+ Note that we'll need to modify this to take in both test and training sets, because we want the returned error to be the test error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set, X_train, y_train, X_test, y_test):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y_train,X_train[list(feature_set)].astype(float))\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X_test[list(feature_set)]) - y_test) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our function to perform forward selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(predictors, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X_train.columns if p not in predictors]\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p], X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "        \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll call our `forward()` function on the training set in order to perform forward selection for all model sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_train = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,len(X.columns)+1):    \n",
    "    models_train.loc[i] = forward(predictors, X[train], y[train][\"Salary\"], X[test], y[test][\"Salary\"])\n",
    "    predictors = models_train.loc[i][\"model\"].model.exog_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the errors, and find the model that minimizes it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(models_train[\"RSS\"])\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RSS')\n",
    "plt.plot(pd.to_numeric(models_train[\"RSS\"]).argmin()+1, models_train[\"RSS\"].min(), \"or\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We find that the best model (according to the validation set approach) is the one that contains 10 predictors.\n",
    "\n",
    "+ Now that we know what we're looking for, let's perform best subset selection on the full dataset and select the best 10-predictor model. \n",
    "+ It is important that we make use of the *full data set* in order to obtain more accurate coefficient estimates. \n",
    "+ We perform this selection on the *full data set* and select the best 10-predictor model, rather than simply using the predictors that we obtained from the training set, because the best 10-predictor model on the full data set may differ from the corresponding model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_test = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,11):    \n",
    "    models_test.loc[i] = forward(predictors, X[test], y[test][\"Salary\"], X[test], y[test][\"Salary\"])\n",
    "    predictors = models_test.loc[i][\"model\"].model.exog_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_trainonly = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,11):    \n",
    "    models_trainonly.loc[i] = forward(predictors, X[train], y[train][\"Salary\"], X[train], y[train][\"Salary\"])\n",
    "    predictors = models_train.loc[i][\"model\"].model.exog_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we see that the best ten-variable model on the full data set has a different set of predictors than the best ten-variable model on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_train.loc[10, \"model\"].model.exog_names)\n",
    "print(models_test.loc[10, \"model\"].model.exog_names)\n",
    "print(models_trainonly.loc[10, \"model\"].model.exog_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Model selection using Cross-Validation\n",
    "+ Now let's try to choose among the models of different sizes using cross-validation.\n",
    "+ This approach is somewhat involved, as we must perform forward selection within each of the $k$ training sets. \n",
    "+ Despite this, we see that with its clever subsetting syntax, `python` makes this job quite easy. \n",
    "+ First, we create a vector that assigns each observation to one of $k = 10$ folds, and we create a DataFrame in which we will store the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10        # number of folds\n",
    "np.random.seed(seed=1)\n",
    "folds = np.random.choice(k, size = len(y), replace = True)\n",
    "\n",
    "# Create a DataFrame to store the results of our upcoming calculations\n",
    "cv_errors = pd.DataFrame(columns=range(1,k+1), index=range(1,20))\n",
    "cv_errors = cv_errors.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Now let's write a for loop that performs cross-validation.\n",
    "+ In the $j^{th}$ fold, the elements of folds that equal $j$ are in the test set, and the remainder are in the training set.\n",
    "+ We make our predictions for each model size, compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix `cv.errors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_cv = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "    \n",
    "# Outer loop iterates over all folds\n",
    "for j in range(1,k+1):\n",
    "\n",
    "    # Reset predictors\n",
    "    predictors = []\n",
    "    \n",
    "    # Inner loop iterates over each size i\n",
    "    for i in range(1,len(X.columns)+1):    \n",
    "    \n",
    "        # The perform forward selection on the full dataset minus the jth fold, test on jth fold\n",
    "        models_cv.loc[i] = forward(predictors, X[folds != (j-1)], y[folds != (j-1)][\"Salary\"], X[folds == (j-1)], y[folds == (j-1)][\"Salary\"])\n",
    "        \n",
    "        # Save the cross-validated error for this fold\n",
    "        cv_errors[j][i] = models_cv.loc[i][\"RSS\"]\n",
    "\n",
    "        # Extract the predictors\n",
    "        predictors = models_cv.loc[i][\"model\"].model.exog_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This has filled up the `cv_errors` DataFrame such that the $(i,j)^{th}$ element corresponds to the test MSE for the $i^{th}$ cross-validation fold for the best $j$-variable model. \n",
    "+ We can then use the `apply()` function to take the `mean` over the columns of this matrix.\n",
    "+ This will give us a vector for which the $j^{th}$ element is the cross-validation error for the $j$-variable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mean = cv_errors.apply(np.mean, axis=1)\n",
    "\n",
    "plt.plot(cv_mean)\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('CV Error')\n",
    "plt.plot(cv_mean.argmin()+1, cv_mean.min(), \"or\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that cross-validation selects a 9-predictor model. Now let's go back to our results on the full data set in order to obtain the 9-predictor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_cv.loc[9, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ For comparison, let's also take a look at the statistics from last lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "# Set up a 2x2 grid so we can look at 4 plots at once\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "plt.plot(models_cv[\"RSS\"])\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RSS')\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "rsquared_adj = models_cv.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rsquared_adj)\n",
    "plt.plot(rsquared_adj.argmax()+1, rsquared_adj.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('adjusted rsquared')\n",
    "\n",
    "# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "aic = models_cv.apply(lambda row: row[1].aic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin()+1, aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "bic = models_cv.apply(lambda row: row[1].bic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin()+1, bic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Notice how some of the indicators are similar the cross-validated model, and others are very different?\n",
    "\n",
    "### YOUR TURN\n",
    "Use any dataset from this link\n",
    "http://archive.ics.uci.edu/ml/datasets.html?format=&task=reg&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrinkage Methods\n",
    "+ Fit a model containing all $p$ predictors.\n",
    "+ The coefficients estimates are constrains or regularizes, which shrink its towards zero.\n",
    "+ It wil reduce the variance, but does not guarantee it will improve the fit.\n",
    "+ Ridge and Lasso regression are most common.\n",
    "\n",
    "### Ridge Regression\n",
    "+ The least squares fitting procedure estimates $\\beta_0, \\beta_1, \\ldots, \\beta_p$ using the values that minimize\n",
    "$$\\textrm{RSS} = \\sum_{i=1}^n{\\left( y_i - \\beta_0 - \\sum_{j=1}^p{\\beta_j x_{ij}} \\right)^2}$$\n",
    "+ Ridge regression minimizes \n",
    "$$\\sum_{i=1}^n{\\left( y_i - \\beta_0 - \\sum_{j=1}^p{\\beta_j x_{ij}} \\right)^2} + \\lambda \\sum_{j=1}^p{\\beta_j^2} = \\textrm{RSS} + \\lambda \\sum_{j=1}^p{\\beta_j^2}$$\n",
    "where $\\lambda \\geq 0$ is a tuning parameter.\n",
    "+ The shrinkage penalty, $\\lambda \\sum_{j=1}^p{\\beta_j^2}$, is small when $\\beta_1, \\ldots, \\beta_p$ are close to zero.\n",
    "+ The tuning parameter $\\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates.\n",
    "+ When $\\lambda=0$ then ridge equals least squares. When $\\lambda \\rightarrow \\infty$, all predictors head to 0.\n",
    "+ Ridge regression produce  a different set of coefficient estimates, $\\hat{\\beta}_\\lambda^R$, for each value of $\\lambda$.\n",
    "+ Since ridge regression works directly with the size of the parameter coefficients, you must scale all predictors by dividing by their standard deviation.\n",
    "$$\\tilde{x}_{ij}=\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n{\\left( x_{ij} - \\bar{x}_j \\right)^2}}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('images/pw46.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ell_2$ norm, $\\left\\Vert \\beta \\right\\Vert_2 = \\sqrt{\\sum_{j=1}^p{\\beta_j^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We will use the `sklearn` package in order to perform ridge regression and the lasso. \n",
    "+ The main functions in this package that we care about are `Ridge()`, which can be used to fit ridge regression models, and `Lasso()` which will fit lasso models. \n",
    "+ They also have cross-validated counterparts: `RidgeCV()` and `LassoCV()`. We'll use these a bit later.\n",
    "\n",
    "Before we proceed, let's first ensure that the missing values have\n",
    "been removed from the data, as described in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/hitters.csv').dropna().drop('Unnamed: 0', axis = 1)\n",
    "df.info()\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform ridge regression and the lasso in order to predict `Salary` on\n",
    "the `Hitters` data. Let's set up our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis = 1)\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The `Ridge()` function has an alpha argument ($\\lambda$, but with a different name!) that is used to tune the model.\n",
    "+ We'll generate an array of alpha values ranging from very big to very small, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Associated with each alpha value is a vector of ridge regression coefficients, which we'll store in a matrix `coefs`. \n",
    "+ In this case, it is a $19 \\times 100$ matrix, with 19 rows (one for each predictor) and 100 columns (one for each value of alpha). \n",
    "+ Remember that we'll want to standardize the variables so that they are on the same scale. To do this, we can use the `StandardScaler()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X_scaled, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "np.shape(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the coefficient estimates to be much smaller, in terms of $l_2$ norm,\n",
    "when a large value of alpha is used, as compared to when a small value of alpha is\n",
    "used. \n",
    "\n",
    "Let's plot and find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the samples into a training set and a test set in order\n",
    "to estimate the test error of ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a ridge regression model on the training set, and evaluate its MSE on the test set, using  $\\lambda=4$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge2 = Ridge(alpha = 4)\n",
    "ridge2.fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred2 = ridge2.predict(X_test)           # Use this model to predict the test data\n",
    "print(pd.Series(ridge2.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred2))          # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let alpha=10^10\n",
    "ridge3 = Ridge(alpha = 10**10)\n",
    "ridge3.fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred3 = ridge3.predict(X_test)           # Use this model to predict the test data\n",
    "print(pd.Series(ridge3.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred3))          # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This big penalty shrinks the coefficients to a very large degree, essentially reducing to a model containing just the intercept.\n",
    "+ This over-shrinking makes the model more biased, resulting in a higher MSE.\n",
    "+ now check whether there is any benefit to performing ridge regression with alpha = 4 instead of just performing least squares regression. \n",
    "+ Recall that least squares is simply ridge regression with alpha = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The least squares, alpha = 0\n",
    "ridge2 = Ridge(alpha = 0)\n",
    "ridge2.fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred = ridge2.predict(X_test)            # Use this model to predict the test data\n",
    "print(pd.Series(ridge2.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred))           # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ It looks like we are indeed improving over regular least-squares!\n",
    "\n",
    "+ Instead of arbitrarily choosing alpha $ = 4$, it would be better to use cross-validation to choose the tuning parameter alpha. \n",
    "+ We can do this using the cross-validated ridge regression function, `RidgeCV()`. \n",
    "+ By default, the function performs generalized cross-validation (an efficient form of LOOCV), though this can be changed using the argument `cv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  use cross-validation to choose the tuning parameter alpha. \n",
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error')\n",
    "ridgecv.fit(X_train, y_train)\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we see that the value of alpha that results in the smallest cross-validation\n",
    "error is 87.37. \n",
    "\n",
    "What is the test MSE associated with this value of\n",
    "alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Fit to the test data\n",
    "ridge4 = Ridge(alpha = ridgecv.alpha_)\n",
    "ridge4.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ridge4.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents a further improvement over the test MSE that we got using\n",
    "alpha $ = 4$. \n",
    "\n",
    "Finally, we refit our ridge regression model on the full data set,\n",
    "using the value of alpha chosen by cross-validation, and examine the coefficient\n",
    "estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Fit to the whole data\n",
    "ridge4.fit(X_scaled, y)\n",
    "\n",
    "print(pd.Series(ridge4.coef_, index = X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Lasso\n",
    "+ Least absolute shrinkage and selection operator\n",
    "+ Ridge regression will include all $p$ predictors in the final model since it will only shrink the coefficients towards zero, not exactly zero. Therefore, if $p$ is large, then the model is hard to be interpreted.\n",
    "+ Lasso can overcome this problem.\n",
    "$$\\sum_{i=1}^n{\\left( y_i - \\beta_0 - \\sum_{j=1}^p{\\beta_j x_{ij}} \\right)^2} + \\lambda \\sum_{j=1}^p{\\left|\\beta_j\\right|} = \\textrm{RSS} + \\lambda \\sum_{j=1}^p{\\left|\\beta_j\\right|}$$\n",
    "\n",
    "+ Lasso uses $\\ell_1$ penalty instead of $\\ell_2$ (absolute value of predictors vs squared value). \n",
    "+ $\\ell_1$ norm, $\\left\\Vert \\beta \\right\\Vert_1 = \\sum{\\left| \\beta_j \\right|}$.\n",
    "+ The lasso performs variable selection by setting some predictors to exactly 0 (and thus automatic variable selection), unlike ridge which will never completely do eliminate variables. \n",
    "+ The lasso yields sparse models — models that involve only a subset of the variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw47.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: \n",
    "+ we saw that ridge regression with a wise choice of alpha can outperform least squares as well as the null model on the Hitters data set. \n",
    "+ We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. \n",
    "+ In order to fit a lasso model, we'll use the `Lasso()` function; however, this time we'll need to include the argument `max_iter = 10000`.\n",
    "+ Other than that change, we proceed just as we did in fitting a ridge model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter = 10000)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Notice that in the coefficient plot that depending on the choice of tuning parameter, some of the coefficients are exactly equal to zero. \n",
    "+ We now perform 10-fold cross-validation to choose the best alpha, refit the model, and compute the associated test error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform 10-fold cross-validation to choose the best alpha.\n",
    "\n",
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is substantially lower than the test set MSE of the null model and of least squares, and only a little worse than the test MSE of ridge regression with alpha chosen by cross-validation.\n",
    "\n",
    "However, lasso has a substantial advantage over ridge regression in\n",
    "that the resulting coefficient estimates are sparse. Here we see that 13 of\n",
    "the 19 coefficient estimates are exactly zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the coefficients are now reduced to exactly zero.\n",
    "pd.Series(lasso.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative logic of lasso and ridge regression\n",
    "+ Instead of thinking of penalizing the error by either the $\\ell_1$ or $\\ell_2$ norm, we can think of setting up lasso/ridge regression as minimizing the squared errors subject to keeping the parameters less than a certain value. \n",
    "+ Think of this value as a 'budget', $s$, of allowable spending to occur. You can allow yourself to spend your parameters in any way you chose as long as you don't go over the total budget.\n",
    "+ For Lasso\n",
    "$$\\displaystyle \\min_{\\beta}\\left\\{ \\sum_{i=1}^n{\\left( y_i - \\beta_0 - \\sum_{j=1}^p{\\beta_j x_{ij}} \\right)^2} \\right\\} \\quad \\textrm{subject to} \\quad \\sum_{j=1}^p{\\left|\\beta_j\\right|} \\leq s$$\n",
    "and for ridge regression\n",
    "$$\\displaystyle \\min_{\\beta}\\left\\{ \\sum_{i=1}^n{\\left( y_i - \\beta_0 - \\sum_{j=1}^p{\\beta_j x_{ij}} \\right)^2} \\right\\} \\quad \\textrm{subject to} \\quad \\sum_{j=1}^p{\\beta^2_j} \\leq s$$\n",
    "\n",
    "+ Lasso yields predictors equivalent to 0 because of sharp corners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ridge and lasso](images/ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is better Lasso or Ridge\n",
    "+ In general, when there are many important predictor variables that related to the response, ridge will perform better. \n",
    "+ When there are only a few variables that relate to the response, lasso will do better.\n",
    "+ However, cross validation can be used to determine this.\n",
    "\n",
    "### Choosing $\\lambda$\n",
    "Choose $\\lambda$ through cross validation. Search an array of $\\lambda$'s through cross validation and choose the $\\lambda$ which minimizes MSE. Then build your model with that $\\lambda$ on all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "\n",
    "1. Now it's time to test out these approaches (ridge regression and the lasso) and evaluation methods (validation set, cross validation) on other datasets. \n",
    "2. You may want to work with a team on this portion of the lab.\n",
    "3. You may use any of the datasets from the UCI machine learning repository (http://archive.ics.uci.edu/ml/datasets.html). \n",
    "4. Download a dataset, and try to determine the optimal set of parameters to use to model it! You are free to use the same dataset you have used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

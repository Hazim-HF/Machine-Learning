{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction Techniques\n",
    "+ Instead of using the original predictors, we transform them first and then fit our models. \n",
    "+ Usually transform variables so that there are less in number than the original set.\n",
    "+ Then fit a least squares model using the transformed variables.\n",
    "+ Let $Z_1, Z_2, \\ldots, Z_M$ represent $M<p$ linear combinations of original $p$ predictors.\n",
    "$$Z_m = \\sum_{j=1}^p{\\phi_{jm}X_j}$$\n",
    "for some constants $\\phi_{1m}, \\phi_{2m}, \\ldots, \\phi_{pm},\\; m=1, \\ldots, M$.\n",
    "+ We can then fit the linear regression model\n",
    "$$y_i = \\theta_0 + \\sum_{m=1}^M{\\theta_m z_{im}} + \\epsilon_i, \\quad i=1, \\ldots, n$$\n",
    "using least squares.\n",
    "+ Notice that\n",
    "$$ \\sum_{m=1}^M{\\theta_m z_{im}} = \\sum_{m=1}^M{\\theta_m}\\sum_{j=1}^p{\\phi_{jm}x_{ij}} = \\sum_{j=1}^p{\\sum_{m=1}^M{\\theta_m\\phi_{jm}x_{ij}}} = \\sum_{j=1}^p{\\beta_jx_{ij}} $$\n",
    "where\n",
    "$$\\beta_j = \\sum_{m=1}^M{\\theta_m\\phi_{jm}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "+ Here we describe its use as a dimension reduction technique for regression.\n",
    "\n",
    "+ The first principal component is the direction where observations vary the most (Fig. 6.14). \n",
    "+ We want to capture as much information as we can in one single direction.\n",
    "+ Which single direction captures as much information as possible? \n",
    "+ The direction where the variance is highest amongst the projected points.\n",
    "\n",
    "+ The first principal component also minimizes the sum of squared perpendicular distances between point and line. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('images/pw48.png', width =700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw49.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The first principal component is\n",
    "$$Z_1 = 0.839 \\times (\\tt{pop} - \\overline{\\tt{pop}}) + 0.544 \\times (\\tt{ad} - \\overline{\\tt{ad}}) $$\n",
    "+ $\\phi_{11} = 0.839$ and $\\phi_{21} = 0.544$\n",
    "+ $\\phi_{11}^2 + \\phi_{21}^2 = 1$\n",
    "+ Since $n = 100$, $\\tt{pop}$ and $\\tt{ad}$ are vectors of length 100, and so is $Z_1$\n",
    "$$z_{i1} = 0.839 \\times (\\tt{pop}_i - \\overline{\\tt{pop}}) + 0.544 \\times (\\tt{ad}_i - \\overline{\\tt{ad}}) $$\n",
    "+ $z_i1$ are the **principal component scores** (Right Fig 6.15).\n",
    "+ Each transformed first principal component can be thought as single number summaries of all that particular observation.\n",
    "+ For this example, if $z_{i1}<0$, this indicates a city with below-average population size and below average ad spending. A positive score suggests the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw50.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The second principal component must be uncorrelated to the first which makes it orthogonal (90 degrees in two dimensions) to the first. \n",
    "+ The second principal component is\n",
    "$$Z_2 = 0.544 \\times (\\tt{pop} - \\overline{\\tt{pop}}) - 0.839 \\times (\\tt{ad} - \\overline{\\tt{ad}}) $$\n",
    "+ The second PC will capture less information (less spread). \n",
    "+ Plotting each PC against each variable can show how much information is captured by each one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/pw51.png', width =700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for the Example:**\n",
    "There is little relationship between the second principal component and these two predictors, suggesting, one only needs the first principal component in order to accurately represent the pop and ad budgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Regression\n",
    "1. Find first M principal components where M < p then fit with least squares. \n",
    "2. Choose M with cross validation. \n",
    "+ Usually, data is standardized by standard deviation first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import PLSRegression, PLSSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/hitters.csv').dropna().drop('Unnamed: 0', axis=1)\n",
    "df.info()\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately `sklearn` does not have an implementation of PCA and regression combined,  so we'll have to do it ourselves.\n",
    "\n",
    "We'll start by performing Principal Components Analysis (PCA), remember to scale the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_reduced = pca.fit_transform(scale(X))   ###scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first few variables of the first few principal components:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_.T).loc[:4,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll perform 10-fold cross-validation to see how it influences the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold CV, with shuffle\n",
    "n = len(X_reduced)\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "regr = LinearRegression()\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    score = -1*model_selection.cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "    \n",
    "# Plot results    \n",
    "plt.plot(mse, '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We see that the smallest cross-validation error occurs when $M = 18$ components are used. \n",
    "+ This is barely fewer than $M = 19$, which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. \n",
    "+ However, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. \n",
    "+ This suggests that a model that uses just a small number of components might suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Amount of variance explained by adding each consecutive principal component:\n",
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We can think of this as the amount of information about the predictors or the response that is captured using $M$ principal components. \n",
    "+ For example, setting $M = 1$ only captures 38.31% of all the variance, or information, in the predictors. \n",
    "+ In contrast, using $M = 6$ increases the value to 88.63%. If we were to use all $M = p = 19$ components, this would increase to 100%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, perform PCA on the training data and evaluate its test set performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca2 = PCA()\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test , y_train, y_test = model_selection.train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# Scale the data\n",
    "X_reduced_train = pca2.fit_transform(scale(X_train))\n",
    "n = len(X_reduced_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept (no principal components in regression)\n",
    "score = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using CV for the 19 principle components, adding one component at the time.\n",
    "for i in np.arange(1, 20):\n",
    "    score = -1*model_selection.cross_val_score(regr, X_reduced_train[:,:i], y_train.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "\n",
    "plt.plot(np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest cross-validation error occurs when  M=6  components are used.\n",
    "\n",
    "Performance on the test data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_test = pca2.transform(scale(X_test))[:,:7]\n",
    "\n",
    "# Train regression model on training data \n",
    "regr = LinearRegression()\n",
    "regr.fit(X_reduced_train[:,:7], y_train)\n",
    "\n",
    "# Prediction with test data\n",
    "pred = regr.predict(X_reduced_test)\n",
    "mean_squared_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Least Squares Regression\n",
    "+ For PCR, the response does not determine the principal components, which means that the PCA is used in an unsupervised way. \n",
    "+ PLSR is a supervised alternative to PCR. \n",
    "+ PLSR generates new features as a linear combination of the old features and the response\n",
    "$$Z_m = \\sum_{j=1}^p{\\phi_{jm}X_j}$$\n",
    "$$y_i = \\theta_0 + \\sum_{m=1}^M{\\theta_m z_{im}} + \\epsilon_i, \\quad i=1, \\ldots, n$$\n",
    "+ Computed by doing simple linear regression of $Y$ onto each predictor and setting that coefficient to the linear combination coefficient for transformed variable $Z_1$. \n",
    "+ So weights are higher for those variables with stronger relationships to response. \n",
    "+ $Z_2$ is computed by regressing all variables against the residuals of $Z_1$ being fit to the model. \n",
    "+ Do this iteratively (fit remaining residuals) to come up with $M$ PLS components. \n",
    "+ Then do least squares fit on all $M$ new dimensions. \n",
    "+ In practice PLSR does not do better than PCR or ridge regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Scikit-learn PLSRegression gives same results as the pls package in R when using method='oscorespls'. \n",
    "##However, the standard method used is 'kernelpls', which we'll use here.\n",
    "\n",
    "n = len(X_train)\n",
    "\n",
    "# 10-fold CV, with shuffle\n",
    "kf_10 = model_selection.KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "mse = []\n",
    "\n",
    "for i in np.arange(1, 20):\n",
    "    pls = PLSRegression(n_components=i)\n",
    "    score = model_selection.cross_val_score(pls, scale(X_train), y_train, cv=kf_10, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(-score)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(np.arange(1, 20), np.array(mse), '-v')\n",
    "plt.xlabel('Number of principal components in regression')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Salary')\n",
    "plt.xlim(xmin=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The lowest cross-validation error occurs when only  M=2  partial least squares dimensions are used.\n",
    "\n",
    "pls = PLSRegression(n_components=3)\n",
    "pls.fit(scale(X_train), y_train)\n",
    "\n",
    "mean_squared_error(y_test, pls.predict(scale(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls.x_weights_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dimensional Data\n",
    "+ When speaking of high dimensional data, we generally mean data with many predictors, especially when p approaches or exceeds n. \n",
    "+ Generally it is better to have more predictors but if many of the predictors are not associated with the response then they can cause the actual signal to get diluted - a double edged sword these predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "\n",
    "## 6a\n",
    "$(y_1 - \\beta_1)^2 + \\lambda\\beta_1^2$\n",
    "\n",
    "Problem allows me to choose $y_1$ and $\\lambda$. I'll choose $y_1 = 5$ and $\\lambda = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot this as a function of beta\n",
    "#(5 - beta)^2 + 3beta^2\n",
    "import numpy as np\n",
    "beta = np.linspace(-10, 10, 1000)\n",
    "y = 5\n",
    "lam = 3\n",
    "ridge = (y - beta)**2 + lam * beta**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(beta, ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min from plot\n",
    "beta[np.argmin(ridge)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min from 6.14\n",
    "y / (1 + lam) # confirmed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6b\n",
    "do similar thing for lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linspace(-10, 10, 1000)\n",
    "y = 5\n",
    "lam = 3\n",
    "lasso = (y - beta)**2 + lam * abs(beta)\n",
    "plt.plot(beta, lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[np.argmin(lasso)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min from 6.15\n",
    "# since y > lambda / 2 minimum should be at y - lambda / 2\n",
    "y - lam / 2 # confirmed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100)\n",
    "err = np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0, beta1, beta2, beta3 = -5, 1, 4, 3\n",
    "y = beta0 + beta1 * x + beta2 * x ** 2 + beta3 * x ** 3 + err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict({'b': 1, 'a':534})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x1': x, 'x2': x ** 2, 'x3': x**3, 'x4': x**4,'x5': x**5,\n",
    "                   'x6': x**6,'x7': x**7,'x8': x**8,'x9': x**9,'x9_10': x**10,\n",
    "                   'y':y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma2 = np.sum((lr.predict(X) - y) ** 2) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best subset selection\n",
    "n = len(X)\n",
    "cp = []\n",
    "bic = []\n",
    "adj_r2 = []\n",
    "for i in range(1, 11):\n",
    "    current_cp = []\n",
    "    current_bic = []\n",
    "    current_adj_r2 = []\n",
    "    for comb in combinations(range(10), i):\n",
    "        X = df.iloc[:, comb]\n",
    "        lr.fit(X, y)\n",
    "        rss = np.sum((lr.predict(X) - y) ** 2)\n",
    "        tss = np.sum((y - y.mean()) ** 2)\n",
    "        d = len(comb)\n",
    "        current_cp.append(1/n * (rss + 2 * d * sigma2))\n",
    "        current_bic.append(1/n * (rss + np.log(n) * d * sigma2))\n",
    "        current_adj_r2.append(1 - rss / (n - d - 1) * (n - 1) / tss)\n",
    "        \n",
    "    cp.append(min(current_cp))\n",
    "    bic.append(min(current_bic))\n",
    "    adj_r2.append(max(current_adj_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1, 11), cp)\n",
    "plt.plot(range(1, 11), bic)\n",
    "plt.title(\"CP and BIC Best subset\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1, 11), adj_r2)\n",
    "plt.title(\"Adjusted R^2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all three agree on correct model!\n",
    "np.argmin(cp), np.argmin(bic), np.argmax(adj_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward selection. Looks at Cp each step and stops if it can't beat old best\n",
    "current_vars = []\n",
    "best_cp = 10000000\n",
    "prev_cp = best_cp\n",
    "best_cp = 1000000\n",
    "while best_cp < prev_cp:\n",
    "    prev_cp = best_cp\n",
    "    old_vars = current_vars.copy()\n",
    "    for i in range(10):\n",
    "        if i in current_vars:\n",
    "            continue\n",
    "        X = df.iloc[:, old_vars + [i]]\n",
    "        lr.fit(X, y)\n",
    "        rss = np.sum((lr.predict(X) - y) ** 2)\n",
    "        d = len(old_vars) + 1\n",
    "        cur_cp = 1/n * (rss + 2 * d * sigma2)\n",
    "        if cur_cp < best_cp:\n",
    "            current_vars = old_vars + [i]\n",
    "            best_cp = cur_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward selection. Looks at Cp each step and stops if it can't beat old best\n",
    "current_vars = list(range(10))\n",
    "best_cp = 10000000\n",
    "prev_cp = best_cp\n",
    "best_cp = 1000000\n",
    "while best_cp < prev_cp:\n",
    "    prev_cp = best_cp\n",
    "    old_vars = current_vars.copy()\n",
    "    for i in range(10):\n",
    "        if i not in current_vars:\n",
    "            continue\n",
    "        old_vars2 = old_vars.copy()\n",
    "        old_vars2.remove(i)\n",
    "        X = df.iloc[:, old_vars2]\n",
    "        lr.fit(X, y)\n",
    "        rss = np.sum((lr.predict(X) - y) ** 2)\n",
    "        d = len(old_vars) + 1\n",
    "        cur_cp = 1/n * (rss + 2 * d * sigma2)\n",
    "        if cur_cp < best_cp:\n",
    "            current_vars = old_vars2.copy()\n",
    "            best_cp = cur_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_vars # same answer for backward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stand = X / X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_stand, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(.0001, .1, 1000)\n",
    "errors = []\n",
    "for alpha in alphas:\n",
    "    ls = Lasso(alpha, max_iter=100000, tol=.0001)\n",
    "    ls.fit(X_train, y_train)\n",
    "    errors.append(np.mean((ls.predict(X_test) - y_test) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas[53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = Lasso(alpha=.0054, max_iter=100000, tol=.0001)\n",
    "ls.fit(X_stand, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.intercept_, ls.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta 3 was very far off\n",
    "beta0, beta1, beta2, beta3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0_7 = 3\n",
    "beta7 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_7 = beta0_7 + beta7 * x ** 7 + err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7 = pd.DataFrame({'x1': x, 'x2': x ** 2, 'x3': x**3, 'x4': x**4,'x5': x**5,\n",
    "                   'x6': x**6,'x7': x**7,'x8': x**8,'x9': x**9,'x9_10': x**10,\n",
    "                   'y':y_7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X['x7'], y_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best subset selection\n",
    "X = df_7.iloc[:, :-1]\n",
    "n = len(X)\n",
    "tss = np.sum((y_7 - y_7.mean()) ** 2)\n",
    "lr.fit(X,  y_7)\n",
    "sigma2 = np.sum((lr.predict(X) - y_7) ** 2) / len(X)\n",
    "cp = []\n",
    "bic = []\n",
    "adj_r2 = []\n",
    "for i in range(1, 11):\n",
    "    current_cp = []\n",
    "    current_bic = []\n",
    "    current_adj_r2 = []\n",
    "    for comb in combinations(range(10), i):\n",
    "        X = df_7.iloc[:, comb]\n",
    "        lr.fit(X, y_7)\n",
    "        rss = np.sum((lr.predict(X) - y_7) ** 2)\n",
    "        \n",
    "        d = len(comb)\n",
    "        current_cp.append(1/n * (rss + 2 * d * sigma2))\n",
    "        current_bic.append(1/n * (rss + np.log(n) * d * sigma2))\n",
    "        current_adj_r2.append(1 - rss / (n - d - 1) * (n - 1) / tss)\n",
    "        \n",
    "    cp.append(min(current_cp))\n",
    "    bic.append(min(current_bic))\n",
    "    adj_r2.append(max(current_adj_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model is with one predictor\n",
    "plt.plot(range(10), cp)\n",
    "bic.append(min(current_bic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso\n",
    "X = df_7.iloc[:, :-1]\n",
    "X_stand = X / X.std()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stand, y_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(.001, 50, 100)\n",
    "errors = []\n",
    "ls = Lasso(alpha, max_iter=1000000000, tol=.000001)\n",
    "\n",
    "for alpha in alphas:\n",
    "    ls = Lasso(alpha=alpha)\n",
    "    ls.fit(X_train, y_train)\n",
    "    errors.append(np.mean((ls.predict(X_test) - y_test) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alphas[np.argmin(errors)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = Lasso(alpha=best_alpha, max_iter=100000, tol=.000001)\n",
    "ls.fit(X_stand, y_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient doesn't resemble model at all. but these have been scaled by\n",
    "# their  std.  must divide by std\n",
    "ls.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's better - very close to actual value of -1\n",
    "ls.coef_ / X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also look at intercept\n",
    "ls.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college = pd.read_csv('data/college.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college['Private'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college['private_yes'] = (college['Private'] == 'Yes') * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = college.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = college['Apps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error\n",
    "np.mean((lr.predict(X_test) - y_test) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = X.iloc[:, :-1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std['private_yes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv = RidgeCV(alphas=np.linspace(.01, 100, 1000), cv=10)\n",
    "rcv.fit(X / X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

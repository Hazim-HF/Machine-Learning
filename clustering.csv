,Model,Y Variable Type,X Variable Type,Task Type,Learning Type,Key Notes,Advantage,Disadvantage,When to Use
0,Linear Regression,Continuous,Numerical,Regression,Supervised,"Assumes linearity, sensitive to multicollinearity","Simple, interpretable; good for understanding relationships",Assumes linearity; sensitive to outliers and multicollinearity,When the relationship between predictors and outcome is approximately linear
1,Logistic Regression,Categorical (binary or multiclass),Numerical,Classification,Supervised,Estimates class probabilities using the logit link,Handles binary/multiclass outcomes; probabilistic interpretation,Requires large sample for stable estimates; may underfit,When predicting categorical outcomes and interpretability is important
2,Linear Discriminant Analysis (LDA),Categorical,Numerical,Classification,Supervised,Assumes normal distribution with shared covariance,Works well with small datasets; interpretable decision boundaries,Assumes normality and equal variance; poor with skewed data,When classes are well-separated and you want interpretability
3,Quadratic Discriminant Analysis (QDA),Categorical,Numerical,Classification,Supervised,Class-specific covariance matrices,Can model more complex boundaries; better for large datasets,Sensitive to outliers; complex estimation of covariance,When class boundaries are nonlinear and data size is sufficient
4,K-Nearest Neighbors (KNN),Categorical or Continuous,Numerical,Both,Supervised,"Lazy learner, no model building",Non-parametric; simple to implement; no training needed,Sensitive to k choice; slow prediction; curse of dimensionality,When the decision boundary is complex and data is not too large
5,Naive Bayes,Categorical,Categorical/Numerical,Classification,Supervised,Assumes feature independence,"Fast, simple; handles mixed types; good with small data",Assumes feature independence; not ideal for correlated features,When you need fast classification and can assume feature independence
6,Decision Trees,Categorical or Continuous,Numerical/Categorical,Both,Supervised,"Prone to overfitting, interpretable",Interpretable; captures nonlinearities; visual decision process,Prone to overfitting; small changes can affect tree structure,When interpretability is critical and dataset is small to medium
7,Random Forests,Categorical or Continuous,Numerical/Categorical,Both,Supervised,"Ensemble of trees, reduces variance",Reduces variance; handles high-dimensional data; robust,Less interpretable; slower; requires tuning,When high accuracy is needed and interpretability is less important
8,Bagging,Categorical or Continuous,Numerical/Categorical,Both,Supervised,Bootstrap aggregation,Reduces variance; handles instability in trees,Can overfit if trees not pruned or limited,When you want to reduce model variance and avoid overfitting
9,Boosting,Categorical or Continuous,Numerical/Categorical,Both,Supervised,Sequential tree training,Boosts weak learners; good accuracy; handles bias/variance,Requires careful tuning; prone to overfitting on noise,When you need high predictive performance and can tune the model
10,Support Vector Machines (SVM),Categorical,Numerical,Classification,Supervised,"Maximizes margin, kernel trick",Effective in high dimensions; robust to overfitting with kernel,Hard to interpret; sensitive to parameter selection,When classes are well-separated and dimensionality is high
11,Principal Component Analysis (PCA),N/A,Numerical,N/A,Unsupervised,Dimensionality reduction,Reduces dimensionality; useful for visualization,Components may be hard to interpret; linear combinations only,When reducing dimensionality or visualizing high-dimensional data
12,Clustering (e.g. K-Means),N/A,Numerical,N/A,Unsupervised,Finds groups of similar points,Finds natural groupings; no labels required,Hard to interpret; sensitive to scale; requires k tuning,When discovering hidden groupings or customer segmentation
13,Neural Networks,Categorical or Continuous,Numerical,Both,Supervised,Flexible nonlinear models,Flexible; captures complex nonlinear patterns,Black-box; needs large data and tuning,"When data is large, nonlinear, and accuracy is the goal"
14,Generalized Additive Models (GAM),Categorical or Continuous,Numerical,Both,Supervised,Nonlinear smoothing components,Combines interpretability with flexibility,Needs smoothers for each feature; may overfit,When needing balance between flexibility and interpretability
15,Lasso Regression,Continuous,Numerical,Regression,Supervised,Adds L1 penalty for feature selection,Performs variable selection; sparse models,Sensitive to noise; may drop important features,"When you need a sparse, interpretable model with many features"
16,Ridge Regression,Continuous,Numerical,Regression,Supervised,Adds L2 penalty to reduce overfitting,Reduces overfitting; handles multicollinearity,Does not do feature selection; biased estimates,When dealing with multicollinearity and overfitting
17,Elastic Net,Continuous,Numerical,Regression,Supervised,Combines L1 and L2 penalties,Balances bias-variance; combines Ridge and Lasso,Needs tuning of two parameters; complex interpretation,When Lasso or Ridge alone underperform
